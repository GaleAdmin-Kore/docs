{"config":{"lang":["tr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2016-2023 Martin Donath</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"agents/configure-an-agent/","title":"Configure an Agent","text":"<p>You can modify and manage the general details of your agent if required and add or manage environment variables. Environment variables allow you to capture commonly used values that different nodes can use. They can be secured and managed from the Agent Configurations page.</p>"},{"location":"agents/configure-an-agent/#add-an-environment-variable","title":"Add an Environment Variable","text":"<p>To add an environment variable, follow these steps:</p> <ol> <li>Click Agents on the top navigation bar of the application. The Agents page is displayed with the list of agents.</li> <li> <p>Click the required Agent you want to modify the details, click Configurations and then click Manage environment variables.</p> <p></p> </li> <li> <p>The Create environment variable dialog box is displayed. Click Add or Add variable.</p> </li> <li> <p>Specify the following information on the Add variable dialog box:</p> <p></p> <ul> <li>Variable name: Provide a descriptive name for the variable.</li> <li>Secure variable: Use the toggle to enable security for the variable. (If you are enabling the security, read the onscreen information carefully to understand the impact.)</li> <li>Value: Enter the desired value for the variable.</li> <li>Notes (optional): Enter any notes about how the variable should be used or its purpose.</li> <li>Click Save. The created variable is listed on the Create environment variable dialog box.</li> </ul> </li> </ol>"},{"location":"agents/configure-an-agent/#edit-or-delete-an-environment-variable","title":"Edit or Delete an Environment Variable","text":"<p>To manage a variable, follow these steps:</p> <ul> <li> <p>Click the three dots icon corresponding to the Name of the variable to edit the details of the variable or Delete to delete a variable.</p> <p></p> </li> </ul> <p>Note</p> <p>To delete an agent, you must first un-deploy it and then delete it.</p>"},{"location":"agents/create-a-new-agent/","title":"Create an Agent","text":"<p>You can create a new agent to leverage models, build flows, and later deploy it as an endpoint. You can start the agent creation journey from the Agents page. The page also allows you to manage your existing agents and view agents that have been shared with you by others.</p> <p>Steps to create an agent:</p> <ol> <li>On the Agents page, click Create new agent. The New agent dialog box is displayed. </li> <li>Enter a name and a brief description for the agent and click Create. The agent is created, and the Agent Flow option is displayed. You can start creating your agent flow now.</li> </ol>"},{"location":"agents/create-a-new-api-key/","title":"Create a New API Key for Agent Access","text":"<p>You can generate an API key for an agent and share it with trusted users to enable secure access to your deployed agent from external environments. It helps ensure that only authorized users from external systems can interact with the agent.</p> <p>Steps to create an API key for your agent:</p> <ol> <li> <p>Navigate to the Agents page and click API keys in the left panel. </p> </li> <li> <p>Click the Create a new API key button. The Create new API key dialog is displayed. </p> </li> <li> <p>Provide a descriptive name for the key and click Generate key. The newly generated API key is displayed in the dialog. Click Copy and close to save the key to your clipboard. You can now share this key with authorized users as needed. </p> <p>Note</p> <p>GALE will not show the API key again for security reasons. Keep the API key secure and confidential. Never share it with unauthorized users or expose it in client-side code or browsers.</p> </li> </ol> <p>All generated API keys are listed in the API keys section for easy reference and management.  </p>"},{"location":"agents/deploy-an-agent/","title":"Deploy an Agent","text":"<p>After completing and testing your agent flow, you can deploy your agent. After the deployment, you can see the dedicated API endpoint created for the agent.</p> <p>Note</p> <p>Before deploying the agent, you must fix any errors or warnings in the Agent Flow.</p> <p>Steps to deploy an agent and get its endpoint:</p> <ol> <li> <p>Click Agent endpoint in the left navigation. </p> </li> <li> <p>Click Deploy. After the deployment, you can see the dedicated API endpoint created for your agent. The API endpoint is available in the following formats: curl, Python, and Node.js. You can copy the code and use it as needed. </p> </li> </ol>"},{"location":"agents/deploy-an-agent/#redeploy-an-agent-for-the-changes-in-the-flow","title":"Redeploy an Agent for the Changes in the Flow","text":"<p>The Deployed version of your agent's flow is accessible from the Agent Flow page. You can click the View the Flow button to view the flow in read-only mode. </p> <p>You can change the in-development version of the flow. To redeploy the agent with the updated flow, click the Deploy button at the top-right corner of the Agent endpoint page. The redeployment doesn't change the agent's endpoint.  </p>"},{"location":"agents/manage-user-roles-and-permissions/","title":"Manage User Roles and Permissions","text":"<p>Account owners can invite users to collaborate on specific Agents. Invited users can access the Models and Data modules for the invited account but can only see the Agents they are invited to. </p> <p>Note</p> <p>You can only invite those users to an application, who have access to your account.</p> <p>Steps to invite users to your Agent:</p> <ol> <li> <p>Open the Agent - click Agents on the top navigation bar and click the specific Agent to open it.</p> </li> <li> <p>Click Sharing &amp; Permission in the left navigation bar. The page lists the existing collaborators, if any. </p> </li> <li> <p>Click Invite to add users to your agent. The Invite Users dialog box is displayed.  </p> <p>Note</p> <p>Only the account owners can view the Invite button and invite other users.    </p> </li> <li> <p>Enter users\u2019 email addresses to invite them and click Invite to grant them access to collaborate on the Agent. </p> </li> </ol>"},{"location":"agents/overview-copy/","title":"About Agents (OLD)","text":"<p>By leveraging AI technology, agents can automate processes, streamline workflows, and generate valuable insights. Integrating these applications seamlessly into existing systems and workflows is the key to maximizing their benefits. For example, creating an agent for deal summarization can help businesses quickly extract essential information from complex contracts or agreements, saving time and reducing the risk of human error. Similarly, an agent for auto email generation can automate routine communication tasks, freeing up employees to focus on more strategic activities.</p> <p>Agents can be accessed using an API key, enabling their easy integration with other enterprise applications that allow for seamless data exchange and interoperability across different systems, enhancing overall connectivity and functionality.</p>"},{"location":"agents/overview/","title":"About Agents","text":"<p>Agents can automate processes, streamline workflows, generate insights, and create outputs like deal summaries or emails. GALE's agent builder empowers you to create AI-powered workflows and automations using a visual drag-and-drop interface with little to no coding required. You can easily configure settings, combine multiple workflows, and integrate with systems like AWS.</p>"},{"location":"agents/overview/#key-features","title":"Key Features","text":"<ul> <li> <p>Agent Flow: A visual no-code/low-code builder for creating and managing agent flows and versions. It uses nodes and transitions to help you automate end-to-end tasks without writing complex code. Learn more.</p> </li> <li> <p>Sharing &amp; Permissions: Account owners can invite users to collaborate on specific Agents. Invited users can access the Models and Data modules for the invited account and see only the Agents they are invited to. Learn more.</p> </li> <li> <p>Agent Endpoint: After building and testing your agent flow, you can deploy it. Once deployed, you'll receive an API endpoint for your agent, which you can use to integrate it with other systems. Learn more.</p> </li> <li> <p>API Keys: Create API keys for your deployed agents to access them from external systems securely. Share these keys only with trusted users. Learn more.</p> </li> <li> <p>Guardrails: Deploy various guardrail models and use them to scan LLM requests and responses to enforce safety and appropriateness standards. Learn more.</p> </li> <li> <p>Configurations: Manage your agent's general details, define reusable values and configurations using environment variables, and undeploy the agent. Learn more.</p> </li> </ul>"},{"location":"agents/agents-flows/flows-overview/","title":"About Agent Flow","text":"<p>GALE's Agent Flow allows you to quickly build an AI agent using a simple drag-and-drop interface combined with smart code editing. This approach enables developers to quickly create a complex AI agent for various use cases, such as candidate evaluation, banking applications, or content generation, without sacrificing flexibility or control.</p> <p>Every flow consists of an input node, which receives the necessary information or variables, and an output node, which provides the results generated by the flow. These results can be accessed by other systems through an API, as every element in the flow automatically becomes part of a deployable API.</p> <p>After building the flow, you can test it to ensure the application functions as intended. For example, you could create an agent for a retail business that collects data on performance, investments, and gross margins. The model can then evaluate key metrics like ROI and provide recommendations for improvement based on the analysis.</p>"},{"location":"agents/agents-flows/manage-flow-nodes/","title":"Manage Nodes in Agent Flow","text":"<p>A flow consists of a sequence of nodes connected on the flow builder canvas. You can easily add nodes on the canvas, connect them, rename and rearrange them, and delete them as required.</p>"},{"location":"agents/agents-flows/manage-flow-nodes/#add-a-new-node","title":"Add a new node","text":"<p>The Start node is the starting point of any flow and is available by default. You can drag and drop it (as well as any other nodes) anywhere on the canvas.</p> <p>Click the \u201c+\u201d icon and select a node from the pop-up menu, or drag a node from the Assets panel onto the canvas.  </p>"},{"location":"agents/agents-flows/manage-flow-nodes/#rename-a-node","title":"Rename a node","text":"<p>Right-click the node you want to rename and select Rename from the list of options. </p>"},{"location":"agents/agents-flows/manage-flow-nodes/#rearrange-nodes","title":"Rearrange nodes","text":"<p>To rearrange nodes on your canvas, drag and drop them to the desired location, as shown in the demo below. </p> <p>Note</p> <ul> <li> <p>Only the node that you are dragging will change position. Any connecting nodes will remain in place, regardless of the direction of their connection.</p> </li> <li> <p>You can right-click anywhere on the canvas and perform some other actions on the canvas such as Changing the background color of the canvas, Show/Hide UI, and Show/Hide Grid by.</p> </li> </ul>"},{"location":"agents/agents-flows/manage-flow-nodes/#connect-nodes","title":"Connect nodes","text":"<p>Node connections are either made from within each node's configuration or by dragging and dropping the nodes on the canvas. You can also connect nodes from the property panel of the node while defining the settings for success and failure. That will also automatically connect the nodes.</p> <p>Steps to connect the nodes:</p> <ol> <li>Click the grey dot that displays next to the node, drag your cursor over the grey dot displayed on top of the connecting node, and release. A line is drawn between the nodes, with an arrow indicating the direction of the node connection.</li> <li>You can change the destination node by clicking the grey dot on top of the initial destination node and then dragging the connection line to the grey dot on top of the new destination node. </li> </ol>"},{"location":"agents/agents-flows/manage-flow-nodes/#delete-a-nodes-connection","title":"Delete a node\u2019s connection","text":"<p>Click the line connecting two nodes and click the Delete icon to delete the connection. The nodes remain on the canvas but will not be connected anymore. To ensure a working flow, you must create another connection between the nodes.</p>"},{"location":"agents/agents-flows/manage-flow-nodes/#delete-a-node","title":"Delete a node","text":"<p>Right-click the required node and select Delete from the list of options.</p>"},{"location":"agents/agents-flows/perform-other-actions-on-the-flow-builder/create-a-new-version-of-the-flow/","title":"Create a New Version of the Flow","text":"<p>You can save versions of your flows, restore the older versions, and delete a version if required. Once you have deployed a flow it will be shown as the Deployed version.</p> <p>To create a new version, follow these steps:</p> <ol> <li> <p>Click the down arrow corresponding to the agents on the header of the canvas.</p> <p>The Flow versions dialog is displayed.</p> </li> <li> <p>Click the + icon to save a version of the flow.</p> <p></p> <p>The Name the Version dialog is displayed.</p> </li> <li> <p>Enter a Name and Description for the flow and click Save.</p> <p></p> <p>The version you saved will be the current version. Any changes you make in the flow are auto-saved to the current version.</p> <p>Note</p> <p>You can click the 3 dots icon corresponding to the version name to Restore or Delete a version. When you restore a version, the current version goes down to the bottom of the list and the restored version becomes the current version and is moved to the top of the list. If you want to edit a restored version, a new current version is created, and you can save this version with a new name. A deployed version can only be restored but cannot be deleted.</p> </li> </ol>"},{"location":"agents/agents-flows/perform-other-actions-on-the-flow-builder/manage-input-and-output/","title":"Manage Input and Output","text":"<p>You can initiate a fresh agent flow and specify the inputs required to initialize the process. Within the starting node, essential variables necessary for the flow's execution are received from the front end through API requests. Correspondingly, the end node gathers outputs from the preceding nodes and transmits them for utilization by the front end as an API.</p> <p>Once the input and output variables have been defined, they become accessible within the context objects and can be utilized throughout the flow's progression.</p> <p>To manage input variables, follow these steps:</p> <ol> <li> <p>Click the Manage I/O button on the top of the canvas of the Agents page.</p> <p>The Manage Input &amp; Output dialog is displayed.</p> <p></p> </li> <li> <p>Click the + Add input variable button under the Input tab.</p> <p>The Enter input variable dialog is displayed. </p> <p></p> </li> <li> <p>Enter a Name(key) to the input variable. For example, Product_ID.</p> </li> <li>Select a data type from the Type drop-down list. If you want to make the data type mandatory, then move the Mandatory toggle button.</li> <li>You can Enable the Default value toggle button to add a Default value for the input variable. You can set a default value for this input variable. If the value is not provided, then the flow automatically selects this default value. For example, Default Value - 9393JAS</li> <li>Click Save. The Input variable is displayed in the list on the Manage Input &amp; Output dialog.</li> </ol> <p>To manage output variables, follow these steps:</p> <ol> <li> <p>Click the Manage I/O button on the top of the canvas of the Agents page.</p> <p>The Manage Input &amp; Output dialog is displayed.</p> </li> <li> <p>Click the + Add output variable button under the Output tab.</p> <p>The Enter output variable dialog is displayed.</p> <p></p> </li> <li> <p>Enter a Name(key) to the input variable.</p> </li> <li>Select a data type from the Type drop-down list.</li> <li>Click Save. The Output variable is displayed in the list on the Manage Input &amp; Output dialog.</li> </ol> <p>Note</p> <p>You can create multiple input as well as output variables based on your use case.</p>"},{"location":"agents/agents-flows/perform-other-actions-on-the-flow-builder/run-the-flow/","title":"Run the Flow","text":"<p>You can run a flow and test it to see how it works. Once you click the Run, a debug log opens and a context object is generated. Every node in the flow has a context temporary storage where you can view what is the context object at each node level.</p> <p>The output shows the start, end, elapsed time taken to run the flow, and if any unresolved issues are displayed.</p> <p>To run the flow, follow these steps:</p> <ol> <li> <p>Click the Run icon on the right side of the top navigation bar of your agents page.</p> <p></p> <p>The debug log is opened and output results are started.</p> </li> <li> <p>Click the Debug icon on the top of the output dialog to open the Debug log as shown in the following image.</p> <p></p> <p>The following are the details you can view:</p> <ul> <li> <p>All the values that you provided in the flow are displayed in this section.</p> </li> <li> <p>The flow level log details are displayed.</p> </li> <li> <p>For each flow initiation, end, and node level details will be displayed.</p> </li> <li> <p>For each node\u2019s success or failure, an info link is displayed. If you click the Info the details are displayed for your reference.</p> </li> <li> <p>If you expand the node, metrics at each node such as Initiated on, Executed on, Total time taken, and Tokens (available only for AI node) are displayed.</p> </li> <li> <p>In case a flow is successful, the copy icon is available to copy the output, and the overall run-time flow is displayed.</p> </li> <li> <p>In case of any errors in the flow, an error message is displayed and the output key in the JSON format displayed is empty as shown in the image.</p> <p></p> </li> </ul> <p>Note</p> <p>You can stop the flow in the middle of a flow and restart the execution by clicking the Run icon again.</p> </li> </ol>"},{"location":"agents/agents-flows/types-of-nodes/api-node/","title":"API Node","text":"<p>The API Node lets you connect to external systems and retrieve data by making SOAP or REST API calls. You can configure the APIs and pass the necessary parameters to fetch the required information.</p>"},{"location":"agents/agents-flows/types-of-nodes/api-node/#add-and-configure-an-api-node","title":"Add and Configure an API Node","text":"<p>Setting up an API node in an Agent Flow involves adding the node at the appropriate location in the flow and configuring various node properties, as explained below.</p> <p>Steps to add and configure the node:</p> <ol> <li> <p>Open the Agent Flow to which you want to add the node: go to Agents &gt; Agent Flow &gt; Go to Flow.</p> </li> <li> <p>The Agent Flow opens in the Flow Builder. Click the \u201c+\u201d icon on any existing node on the canvas and select API from the pop-up menu. (Alternatively, you can drag the API node from the Assets panel onto the canvas.)</p> </li> <li> <p>Click the added node to open its properties dialog box. The General Settings for the node are displayed. </p> </li> <li> <p>Enter or select the following information:</p> <ul> <li>Custom Name: Enter an appropriate name for the node.</li> <li>Type: Select the API type from the drop-down list - REST or SOAP.</li> <li>Request Definition: Define the details of the service request to make the call and fetch the data. Click Define Request and enter or select the following details in the Edit Request dialog box: <ol> <li>Select the request type from the list - GET, PUT, POST, PATCH, or DELETE.</li> <li>Paste your API Endpoint URL or cURL in the text field.</li> <li>In the Headers tab, specify the details of the Key and Value pair. For example,  Key: Content-Type Value: application/json</li> <li>The Body tab is displayed for all request types except GET. Select the body content type from the drop-down list:<ul> <li>application/x-www-form-urlencoded: Allows file uploads through HTTP POST requests. Add key/value pairs encoded by the platform.</li> <li>application/json: Transmits data between servers and web applications using JSON format without processing.</li> <li>application/xml: Sends XML payload for SOAP services via POST methods, with the option to include node values.</li> <li>Custom: Allows sending request payload in non-standard formats, such as for handling blogs or custom variables. </li> </ul> </li> <li>Click the Test button at the top-right corner of the dialog. The API response will appear on the Response tab.</li> <li>Click Save at the top-right corner of the dialog.</li> </ol> </li> </ul> </li> <li> <p>Click the Connections icon in the left navigation and select the Go to Node for both success and failure conditions. </p> <ol> <li> <p>On Success &gt; Go to Node: After the current node is successfully executed, go to a selected node in the flow to execute next. For example, you can process the data from this node into a Function node and then use it further. In this case, select the Function node. </p> </li> <li> <p>On Failure &gt; Go to Node: If the execution of the current node fails, go to an appropriate node having a custom error message configured.</p> </li> </ol> </li> <li> <p>Finally, test the flow and fix any issues found: Click the Run Flow button at the top-right corner of the flow builder.</p> </li> </ol> <p>Note</p> <p>To access an API node using the context variable, use the following format:  <pre><code>{{context.steps.Start.APINodeName}}\n</code></pre></p>"},{"location":"agents/agents-flows/types-of-nodes/api-node/#api-status-codes","title":"API Status Codes","text":"<ul> <li>200: Request successful</li> <li>400: Client error, cannot process request</li> <li>401: Authentication required</li> <li>403: Access denied</li> <li>404: Resource not found</li> <li>500: Unexpected server error</li> <li>504: Gateway timeout, no timely response from the upstream server</li> </ul>"},{"location":"agents/agents-flows/types-of-nodes/condition-node/","title":"Condition Node","text":"<p>Condition Nodes allow you to create branches in a workflow, directing actions based on whether certain conditions are met. This helps control the flow\u2019s execution.</p> <p>The node can handle three types of conditions:</p> <ul> <li>If: Directs the flow to a specific path if certain criteria (Node, Context, or Value) are met.</li> <li>Else: Sets the node connection when no condition is met, allowing you to configure the next node to connect to.</li> <li>Else If: Allows you to configure another set of criteria (Node, Context, or Value) to be met when the initial If condition is not satisfied.</li> </ul> <p>Note</p> <p>Due to security reasons, a condition can be called a maximum of 10 times in an agent flow. Exceeding this limit will result in an error.</p>"},{"location":"agents/agents-flows/types-of-nodes/condition-node/#add-and-configure-a-condition-node","title":"Add and Configure a Condition Node","text":"<p>Setting up a Condition node in an Agent Flow involves adding the node at the appropriate location in the flow and configuring various node properties, as explained below.</p> <p>Steps to add and configure the node:</p> <ol> <li> <p>Open the Agent Flow to which you want to add the node: go to Agents &gt; Agent Flow &gt; Go to Flow.</p> </li> <li> <p>The Agent Flow opens in the Flow Builder. Click the \u201c+\u201d icon on any existing node on the canvas and select Condition from the pop-up menu. (Alternatively, you can drag the Condition node from the Assets panel onto the canvas.)</p> </li> <li> <p>Click the added node to open its properties dialog box. The General Settings for the node are displayed. </p> </li> <li> <p>Enter or select the following information:</p> <ul> <li>Custom Name: Enter an appropriate name for the node.</li> <li> <p>If/Else Condition: Define the IF ELSE/ELSE IF conditions using context variables and the AND/OR operator. You can use Node, Context, or Value.</p> <ol> <li> <p>In the IF section, select the context variable you want to use - enter \"{{context.\" and select the node/variable from the list and then close the braces with \"}}\". </p> <p>For example:  <pre><code>{{context.ambiguous_sub_categories}}\n</code></pre></p> </li> <li> <p>Select an appropriate Operator from the drop-down list. For example, Contains.</p> </li> <li> <p>Enter the value for the condition. If you want to use a context variable, start entering \"{{context.\" and then select the node/variable, and then close the braces with \"}}\".</p> <p>For example: <pre><code>{{context.steps.Sub_Category_Detection.output}}   \n</code></pre></p> </li> <li> <p>Additionally, you can use an AND/OR logical operator to add more criteria to the condition.</p> </li> <li> <p>In the Then Go To drop-down list, select the node to connect to if the condition is met.</p> </li> <li> <p>In the ELSE section, select the node you want to trigger if the IF condition fails. </p> </li> </ol> </li> </ul> </li> <li> <p>Finally, test the flow and fix any issues found: Click the Run Flow button at the top-right corner of the flow builder.</p> </li> </ol> <p>Standard Error</p> <p>If a condition is true or false but has no connected node, the following error message is displayed: \"Path not defined. Please check the flow.\"</p>"},{"location":"agents/agents-flows/types-of-nodes/end-node/","title":"End Node","text":"<p>End nodes allow you to show the agent\u2019s/flow's outputs on success or an error message on failure.   </p>"},{"location":"agents/agents-flows/types-of-nodes/end-node/#add-and-configure-an-end-node","title":"Add and Configure an End Node","text":"<p>Setting up an End node in an Agent Flow involves adding the node at the appropriate location in the flow and configuring various node properties, as explained below.</p> <p>Steps to add and configure the node:</p> <ol> <li> <p>Open the Agent Flow to which you want to add the node: go to Agents &gt; Agent Flow &gt; Go to Flow.</p> </li> <li> <p>The Agent Flow opens in the Flow Builder. Click the \u201c+\u201d icon on any existing node on the canvas and select End from the pop-up menu. (Alternatively, you can drag the End node from the Assets panel onto the canvas.)</p> </li> <li> <p>Click the added node to open its properties dialog box. The General Settings for the node are displayed. </p> </li> <li> <p>Enter or select the following information:</p> <ul> <li> <p>Custom Name: Enter an appropriate name for the node.</p> </li> <li> <p>Name (key): Select a key from the drop-down list. All defined keys in the Manage Output section are displayed here. You can select a variable and assign a value to it. (You can also add a new key. For more information, see Manage Input and Output variable.) </p> </li> <li> <p>Value: Select an appropriate variable or node as the value. Enter \u201c{{context.\u201d and select the node/variable from the list and then close the braces with \u201c}}\u201d.</p> <p>For example: <pre><code>{{context.steps.summarization.output}}\n</code></pre> </p> </li> <li> <p>If you want to show multiple outputs/messages, click Add a Key to add the key and value details for the same.</p> </li> </ul> </li> <li> <p>Finally, test the flow and fix any issues found: Click the Run Flow button at the top-right corner of the flow builder.</p> </li> </ol> <p>Standard Error</p> <p>When the value for the output variable is not defined, a list of unresolved outputs is displayed.</p>"},{"location":"agents/agents-flows/types-of-nodes/function-node/","title":"Function Node","text":"<p>Function nodes allow you to write custom scripts using JavaScript or Python to process context variables or other variables used in building the experience flow.</p>"},{"location":"agents/agents-flows/types-of-nodes/function-node/#add-and-configure-a-function-node","title":"Add and Configure a Function Node","text":"<p>Setting up a Function node in an Agent Flow involves adding the node at the appropriate location in the flow and configuring various node properties, as explained below.</p> <p>Steps to add and configure the node:</p> <ol> <li> <p>Open the Agent Flow to which you want to add the node: go to Agents &gt; Agent Flow &gt; Go to Flow.</p> </li> <li> <p>The Agent Flow opens in the Flow Builder. Click the \u201c+\u201d icon on any existing node on the canvas and select Function from the pop-up menu. (Alternatively, you can drag the Function node from the Assets panel onto the canvas.)</p> </li> <li> <p>Click the added node to open its properties dialog box. The General Settings for the node are displayed. </p> </li> <li> <p>Enter or select the following information:</p> <ul> <li> <p>Custom Name: Enter an appropriate name for the node.</p> </li> <li> <p>Define a Script: You can define a script using Javascript or Python to process/parse the given input or the output of the previous node. Click anywhere in the Define a Script field to open the Script Editor dialog box. </p> <ol> <li> <p>In the top-left corner, choose the appropriate scripting language - Javascript or Python. Enter the script in the box. You can use variables or context variables in the Script Editor. For example:</p> <p>To read an input variable: <pre><code>context.&lt;variable-name&gt; = context.steps.&lt;startNode-Name&gt;.&lt;inputVariable-name&gt;\n</code></pre> To call a function: <pre><code>context.&lt;UserDefined-Variable-Name&gt;= UserDefined-Function-Name(context.steps.&lt;startNode-Name&gt;.&lt;inputVariable-name&gt;\n</code></pre></p> <p>Note</p> <p>You cannot import packages in the Function node. However, you can use pre-existing libraries such as pandas or  NumPy.</p> </li> <li> <p>You can use the Context input/output feature to use a variable from the previous node or when testing a function.</p> <ul> <li>Context Input: Add the required variable; also, dummy input values can be given to test the defined function.</li> <li>Context Output: Shows the results of the function from the given input.</li> <li>Log: Monitor the state of the function and view the values.</li> </ul> </li> <li> <p>Click the Run button to test the script. Once the script is resolved successfully, the results are displayed in the Log section.</p> </li> <li>Close the Script Editor.</li> </ol> </li> </ul> </li> <li> <p>Click the Connections icon in the left navigation and select the Go to Node for both success and failure conditions. </p> <ol> <li>On Success &gt; Go to Node: After the current node is successfully executed, go to a selected node in the flow to execute next. For example, you can go to a Gen AI node to use the processed data from the Function node. </li> <li>On Failure &gt; Go to Node: If the execution of the current node fails, go to an appropriate node having a custom error message configured for this node.</li> </ol> </li> <li> <p>Finally, test the flow and fix any issues found: Click the Run Flow button at the top-right corner of the flow builder.</p> </li> </ol> <p>Standard Errors</p> <p>You can see compilation and runtime errors, if any, during the execution of the script/node.</p>"},{"location":"agents/agents-flows/types-of-nodes/gen-ai-node/","title":"Gen AI Node","text":"<p>The Gen AI Node is a key component in GALE's Agent Flow that allows users to leverage LLMs for specific use cases. The node processes inputs and generates responses, which can be integrated into larger workflows within the Agent Flow.</p>"},{"location":"agents/agents-flows/types-of-nodes/gen-ai-node/#add-and-configure-a-gen-ai-node","title":"Add and Configure a Gen AI Node","text":"<p>Setting up a Gen AI node in an Agent Flow involves adding the node at the appropriate location in the flow and configuring various properties of the node, as explained below.</p> <p>Steps to add and configure the node:</p> <ol> <li> <p>Open the Agent Flow to which you want to add the node: go to Agents &gt; Agent Flow &gt; Go to Flow.  </p> </li> <li> <p>The Agent Flow opens in the Flow Builder. Click the \u201c+\u201d icon on any existing node on the canvas and and select Gen AI from the pop-up menu. (Alternatively, you can drag the Gen AI node from the Assets panel onto the canvas.)  </p> </li> <li> <p>Click the added node to open its properties dialog box. The General Settings for the node are displayed. </p> </li> <li> <p>Enter or select the following General Settings:</p> <ul> <li> <p>Custom Name: Enter an appropriate name for the node.</p> </li> <li> <p>Select Model: Select a model from the list of configured models. (For more information on models, see Model Studio.)</p> </li> <li> <p>System Prompt: Enter the System Prompt for your use case.  For example, \u201cGenerate a summary of the transcription of a conversation in a maximum of 5 lines without returning any special characters.\u201d</p> </li> <li> <p>Prompt: It allows you to pass a variable to the System Prompt. For example, you can store the conversation transcript in a variable named \u201cconversation\u201d and pass it on in the Prompt. Format:      <pre><code>{{context.variable_name}}\n</code></pre>      Example:    <pre><code>{{context.conversation}}\n</code></pre></p> </li> <li> <p>Examples: Add a few relevant examples to guide the model. Click the arrow to add examples of User input and expected AI output. </p> </li> <li> <p>Hyperparameters: Hyperparameters allow you to fine-tune the AI model's behavior to suit your needs. While the default settings work well for most cases, you can adjust them to find the right balance for your use case.</p> <ul> <li> <p>Temperature: Controls the randomness of the model's responses. Higher values lead to more random outputs, while lower values result in more focused outputs.</p> </li> <li> <p>Max Tokens: Sets the maximum length of the model's output. Lower values generate shorter responses, while higher values produce longer responses.</p> </li> <li> <p>Frequency Penalty: Penalizes common or frequent tokens, making the model's output less generic. Higher values produce more unique outputs, while lower values result in more common outputs.</p> </li> <li> <p>Presence Penalty: Penalizes new or rare tokens, making the model's output more common. Higher values lead to more common outputs, while lower values result in more unique outputs.</p> </li> <li> <p>Top P: Controls the diversity of the model's output by considering only the top tokens whose cumulative probability exceeds a threshold. Higher values produce more diverse outputs, while lower values result in more deterministic outputs.</p> </li> </ul> </li> </ul> </li> <li> <p>Click the Connections icon and select the Go to Node for both success and failure conditions. </p> <ol> <li> <p>On Success &gt; Go to Node: After the current node is successfully executed, go to a selected node in the flow to execute next, such as a Gen AI node, Function node, Condition node, API node, or End node.</p> </li> <li> <p>On Failure &gt; Go to Node: If the execution of the current node fails, go to the End node to display any custom error message from the Gen AI node.</p> </li> </ol> </li> <li> <p>Finally, test the flow and fix any issues found: Click the Run Flow button at the top-right corner of the flow builder.</p> </li> </ol> <p>Standard Error</p> <p>When the Model is not selected, the prompt details are not provided, or both, the following error message is displayed: \u201cProper data needs to be provided in the LLM node\u201d.</p>"},{"location":"agents/agents-flows/types-of-nodes/gen-ai-node/#access-the-output-of-the-gen-ai-node","title":"Access the Output of the Gen AI Node","text":"<p>The output of the Gen AI node is stored in a context variable and can be accessed via  <pre><code>{{context.steps.GenAINodeName.output}}\n</code></pre></p> <p>Note</p> <p>GALE can automatically recognize variables/outputs. To do this, just type \"context.steps.\" and you will see available context variables/nodes, including the nodes' outputs.</p>"},{"location":"agents/guardrails/add-a-scanner/","title":"Add Input/Output Scanners","text":"<p>You can add input or output scanners to evaluate prompts or responses in your application. Input scanners assess inputs or prompts sent to the LLM node, while output scanners evaluate the responses received from the LLM.</p> <p>In the following steps, you will learn how to add an input scanner. The steps to add an output scanner are similar to those for an input scanner. You need to add input and output scanners separately based on your requirements.</p> <p>Note</p> <p>Scanners must be deployed before you add or use them. Learn more.</p> <p>Steps to add a Scanner:</p> <ol> <li> <p>Click Agents on the top navigation bar and select the desired agent.</p> </li> <li> <p>Click Guardrails in the left navigation pane. The Guardrails page is displayed. </p> </li> <li> <p>In the Input Scanners section, click Add Scanner, select the required scanners from the pop-up list, and then click Done.  </p> <p>The selected scanners are added to the list. </p> </li> <li> <p>Click an added scanner to configure its settings, which vary depending on the scanner type. For example, Toxicity has \"Threshold\" and \"End the flow if the risk score is above\" settings, while Regex has \"Enter patterns to ban\", \"End the flow if the risk score is above\", and \"Match type\" settings. </p> </li> <li> <p>(Optional) To add more scanners, click the plus or Add Scanner icon. To remove an unused scanner, click the minus or Remove Scanner icon. </p> </li> </ol>"},{"location":"agents/guardrails/manage-guardrails/","title":"Manage Guardrails","text":"<p>To use input and output scanners in GALE, you need to deploy them first. Once deployed, a scanner will be available across all your agents on the platform.</p> <p>Steps to deploy a scanner:</p> <ol> <li> <p>Click the Settings icon in the top navigation bar.</p> </li> <li> <p>In the left navigation pane, click Manage Guardrails. </p> </li> <li> <p>On the Manage guardrail models page, click the Deploy button next to the scanner you want to deploy. </p> <p>The deployment process starts, and the status is displayed as Deploying. Once the scanner is deployed, it's available in the scanners list when a new scanner is added.</p> <p>Note</p> <p>If you no longer need a scanner, click the Undeploy button next to the scanner to remove it from the platform.</p> </li> </ol>"},{"location":"agents/guardrails/overview/","title":"About Guardrails","text":"<p>Guardrails are safety measures that ensure AI-generated responses from large language models (LLMs) are appropriate and align with standards. You can deploy various guardrail models in GALE and use them to scan the inputs or prompts and output results. The scanners ensure responsible AI interactions while generating responses.</p> <p>Supported Scanners:</p> <p>Regex Scanner </p> <ul> <li>Validates prompts based on user-defined regular expression patterns.</li> <li>Allows defining desirable (\"good\") and undesirable (\"bad\") patterns for fine-grained prompt validation.</li> </ul> <p>Anonymize Scanner </p> <ul> <li>Ensures user prompts remain confidential by removing sensitive data.</li> <li>Helps maintain user privacy and prevents exposure of personal information.</li> </ul> <p>Ban Topics Scanner </p> <ul> <li>Restricts specific topics, such as religion, from being introduced in prompts.</li> <li>Maintains acceptable boundaries and avoids potentially sensitive or controversial discussions.</li> </ul> <p>Prompt Injection Scanner </p> <ul> <li>Protects LLM against crafty input manipulations.</li> <li>Identifies and mitigates injection attempts to ensure secure LLM operation.</li> </ul> <p>Toxicity Scanner </p> <ul> <li>Analyzes and gauges the toxicity level of prompts.</li> <li>Assists in maintaining healthy and safe online interactions by preventing the dissemination of potentially harmful content.</li> </ul> <p>Bias Detection Scanner</p> <ul> <li>Inspects LLM-generated outputs to detect and evaluate potential biases.</li> <li>Ensures LLM outputs remain neutral and free from unwanted or predefined biases.</li> </ul> <p>Deanonymize Scanner </p> <ul> <li>Replaces placeholders in the model's output with real values.</li> <li>Helps restore original information in the output when needed.</li> </ul> <p>Relevance Scanner </p> <ul> <li>Measures the similarity between the input prompt and the model's output.</li> <li>Provides a confidence score indicating the contextual relevance of the response.</li> <li>Ensures LLM outputs remain aligned with the given input prompt.</li> </ul>"},{"location":"agents/guardrails/test-guardrail/","title":"Test Guardrails","text":"<p>After adding and configuring the necessary scanners, you can verify that they adhere to the specified standards. Test the effectiveness of an individual scanner or a group of scanners and then tweak the scanner's settings if required.</p> <p>Steps to test the Guardrails or Scanners:</p> <ol> <li> <p>On the Guardrails page, click Test. </p> </li> <li> <p>On the Test Guardrails page, in the Prompt input box, enter a prompt or select Input template to choose a template. </p> </li> <li> <p>Select Test. Under Scores and Results, review the following results: </p> <ul> <li> <p>Validity: Indicates whether the prompt is valid based on the scanner's criteria. (For example, if Toxicity is not found in the prompt, the prompt is valid, and the Validity is set to True.)</p> </li> <li> <p>Risk Score: Indicates the prompt's risk level, calculated using the scanner's threshold and score.</p> </li> <li> <p>Duration: Displays the time taken by the scanner to process the prompt.</p> </li> </ul> <p>Note</p> <ul> <li> <p>The Risk Score is calculated using the following formula: (Threshold - Scanner Score) / Threshold. </p> </li> <li> <p>For the Relevance Scanner, if the similarity between the prompt and generated answer is below the user-defined threshold, the Risk Score is 1; otherwise, it's 0.</p> </li> </ul> </li> <li> <p>Based on the results, you can adjust the Scanner's settings and test them again if required.</p> </li> </ol>"},{"location":"data/overview/","title":"Overview","text":"<p>The data module streamlines data management by allowing you to store your datasets in the application instead of uploading them every time you test prompts in the playground or upload files for fine-tuning the model. Additionally, the module provides flexibility to store files of different file formats such as CSV, JSON, and JSONL without any additional formatting requirements, ultimately enhancing convenience for you while working with large datasets within the product.</p>"},{"location":"data/upload-a-dataset/","title":"Upload a Dataset","text":"<p>You can upload a dataset of your choice in CSV, JSON, or, JSONL format.</p> <p>To upload a dataset, follow these steps:</p> <ol> <li> <p>Click Data on the top navigation bar of the application. The Data page is displayed.</p> <p></p> </li> <li> <p>Click the Upload dataset button to upload your data from the location. The uploaded file is listed on the Data page.</p> <p></p> <p>Note</p> <p>When you Upload a file in the Playground and the fine-tuning wizard the uploaded file is saved under the Data tab and can be used later. Also, the dataset you upload here is displayed for you to choose in the training dataset section of the fine-tuning wizard.</p> </li> <li> <p>When you select the Advance mode in the Prompt Playground and click the Add dataset option you can view and select the dataset you have added from the list as shown in the following image.</p> <p></p> </li> </ol> <p>Note</p> <p>Click the three dots icon corresponding to the name of the dataset and select Delete to delete the dataset if you no longer require it and select Download to download the dataset. However, if you are using a data file in the Playground section and delete the file from the Data page the Playground experiments are impacted, and an error message is displayed. </p>"},{"location":"getting-started/introduction-copy/","title":"Introducing All-new GALE Platform","text":"<p>GALE is a new platform for building LLM-powered AI agents. It provides tools to integrate all the pieces necessary to power Gen AI agents that can be seamlessly integrated into your existing systems and workflows, enabling you to quickly unlock the benefits of Gen AI.</p> <p>With GALE's user-friendly drag-and-drop interface, you can easily prototype, customize, and deploy AI agents tailored to your specific needs. The platform offers a range of popular open-source and commercial AI models that you can fine-tune as required. You can also chain together workflows, manage inputs/outputs, and implement guardrails - all without the need for coding expertise.</p> <p>Once your AI agent is ready, GALE simplifies the deployment process via API, allowing you to seamlessly integrate it into your existing systems and applications. This streamlined approach to leveraging Gen AI capabilities makes GALE the perfect solution for any business looking to accelerate their AI adoption.</p>"},{"location":"getting-started/introduction-copy/#key-components-of-gale","title":"Key Components of GALE","text":"<p>Agents, Models, and Playground are the key components of GALE that work together to enable businesses to leverage AI capabilities effectively.</p> AgentsModelsPlayground <p>Agents allow you to create AI-powered workflows and automation with little to no coding required. Using a visual drag-and-drop interface, you can easily configure settings and combine multiple complex workflows. Guardrails ensure the models within these workflows operate responsibly, adhering to societal norms and your business requirements. The agents can seamlessly integrate with AWS, expanding the range of use cases. This user-friendly approach empowers you to leverage AI capabilities across various applications without extensive technical expertise. Learn more about Agents </p> <p>Models are the core of GALE. Based on your needs, you can choose from fine-tuned, commercial, or open-source AI models. Once you've selected the right models, you can easily integrate them into your AI agents. Guardrails ensure the models generate outputs responsibly and follow defined constraints. Additionally, you can deploy these AI agents via API endpoints and integrate them with your existing systems as required. Learn more about Models </p> <p>GALE's Prompt Playground allows you to experiment and refine prompts to get the best performance from AI models. You can test different prompts across various models - external, fine-tuned, or open-source. The Playground helps you identify the ideal model and configurations for each prompt through an iterative process. This streamlined workspace enables you to optimize prompts rapidly for maximum model effectiveness. Learn more about Playground </p>"},{"location":"getting-started/introduction-copy/#deployment-of-ai-at-scale","title":"Deployment of AI at Scale","text":"<p>GALE provides enterprise-grade features to deploy AI at scale:</p> <ul> <li> <p>Security: Your data and intellectual property are protected by robust security measures.</p> </li> <li> <p>Scalability: As your AI applications grow and user numbers increase, GALE automatically scales to meet demand while maintaining consistent performance.</p> </li> <li> <p>Flexible Deployment: Based on your organization's preferences, you can deploy AI models on the cloud or on-premises infrastructure.</p> </li> </ul> <p>With GALE's enterprise-ready capabilities, you can confidently scale your AI adoption while ensuring data security, reliable performance, and deployment flexibility to suit your unique business needs.</p>"},{"location":"getting-started/introduction/","title":"Introducing All-new GALE Platform","text":"<p>GALE is a new platform for building LLM-powered AI agents. It provides tools to integrate all the pieces necessary to power Gen AI agents that can be seamlessly integrated into your existing systems and workflows, enabling you to quickly unlock the benefits of Gen AI.</p> <p></p> <p>You don't need to be an AI expert or write complex code. The platform provides ready-to-use AI models and intuitive tools to create custom Gen AI solutions quickly.</p> <p>With GALE's user-friendly drag-and-drop interface, you can easily prototype, customize, and deploy AI agents tailored to your specific needs. The platform offers a range of popular open-source and commercial AI models that you can fine-tune as required. You can also chain together workflows, manage inputs/outputs, and implement guardrails - all without the need for coding expertise.</p> <p>Once your AI agent is ready, GALE simplifies the deployment process via API, allowing you to seamlessly integrate it into your existing systems and applications. This streamlined approach to leveraging Gen AI capabilities makes GALE the perfect solution for any business looking to accelerate their AI adoption.</p>"},{"location":"getting-started/introduction/#key-components-of-gale","title":"Key Components of GALE","text":"<p>Agents, Models, and Playground are the key components of GALE that work together to enable businesses to leverage AI capabilities effectively.</p> AgentsModelsPlayground <p>Agents allow you to create AI-powered workflows and automation with little to no coding required. Using a visual drag-and-drop interface, you can easily configure settings and combine multiple complex workflows. Guardrails ensure the models within these workflows operate responsibly, adhering to societal norms and your business requirements. The agents can seamlessly integrate with AWS, expanding the range of use cases. This user-friendly approach empowers you to leverage AI capabilities across various applications without extensive technical expertise.  </p> <p></p> <p>Learn more about Agents </p> <p>Models are the core of GALE. Based on your needs, you can choose from fine-tuned, commercial, or open-source AI models. Once you've selected the right models, you can easily integrate them into your AI agents. Guardrails ensure the models generate outputs responsibly and follow defined constraints. Additionally, you can deploy these AI agents via API endpoints and integrate them with your existing systems as required. </p> <p></p> <p>Learn more about Models </p> <p>GALE's Prompt Playground allows you to experiment and refine prompts to get the best performance from AI models. You can test different prompts across various models - external, fine-tuned, or open-source. The Playground helps you identify the ideal model and configurations for each prompt through an iterative process. This streamlined workspace enables you to optimize prompts rapidly for maximum model effectiveness.  </p> <p></p> <p>Learn more about Playground </p>"},{"location":"getting-started/introduction/#deployment-of-ai-at-scale","title":"Deployment of AI at Scale","text":"<p>GALE provides enterprise-grade features to deploy AI at scale:</p> <ul> <li>Security: Your data and intellectual property are protected by robust security measures.</li> <li>Scalability: As your AI applications grow and user numbers increase, GALE automatically scales to meet demand while maintaining consistent performance.</li> <li>Flexible Deployment: Based on your organization's preferences, you can deploy AI models on the cloud or on-premises infrastructure.</li> </ul> <p>With GALE's enterprise-ready capabilities, you can confidently scale your AI adoption while ensuring data security, reliable performance, and deployment flexibility to suit your unique business needs.</p>"},{"location":"models/overview/","title":"About Models","text":"<p>GALE's Models Studio empowers you to enhance base models by fine-tuning them directly within the product using your enterprise's proprietary data. This allows you to create customized models tailored to your specific needs. You can also seamlessly integrate models from external sources and open-source models, expanding the range of available models to address your unique requirements.</p>"},{"location":"models/overview/#key-features","title":"Key Features","text":"<ul> <li> <p>Fine-tuned model: Customize existing models to suit your specific requirements. Learn more.</p> </li> <li> <p>Open-source model: Choose from a curated list of 30+ popular open-source models or bring in any text generation model from Hugging Face. Learn more.</p> </li> <li> <p>External models: Add commercial models like OpenAI, Anthropic, Azure OpenAI, Cohere, and Google. You can also integrate your own hosted models into GALE using API connections. Learn more.</p> </li> <li> <p>Quick deployment: Deploy a model quickly in just a few steps - select the model, review parameters and hardware, and click Deploy. Learn more.</p> </li> </ul>"},{"location":"models/external-models/add-an-external-model-using-api-integration/","title":"Add an External Model using API Integration","text":"<p>Custom API Integration in GALE enables extensibility of use, where it allows you to bring in your models from an external ecosystem via API integration.</p> <p>To add an external model using API integration, follow these steps:</p> <ol> <li> <p>Click Models on the top navigation bar of the application. The Models page is displayed.</p> </li> <li> <p>Click the External models tab on the Models page. </p> </li> <li> <p>Click Add a model under the External models tab. The Add an external model dialog is displayed. </p> </li> <li> <p>Select the Custom integration option to connect models via API integration, and click Next.    The Custom API integration dialog is displayed.</p> </li> <li> <p>Enter a Model name and Model endpoint URL in the respective fields. </p> </li> <li> <p>In the Headers section, specify the headers that need to be sent along with the request payload such as Key and Value. </p> </li> <li> <p>In the Variables section, you have 2 types:</p> <ul> <li>Prompt variable: The Prompt variable is by default set to mandatory. You can Turn ON the toggle button for the System prompt and examples if required.</li> <li>Custom variables: Click the Custom variables tab under the Variables section and click the +Custom variables button on the Custom variables section. </li> </ul> <p>The Add custom variable dialog is displayed. Enter the Variable name, and Display name and select the Data type. </p> </li> <li> <p>In the Body section, add request body of the model you are trying to connect with in GALE. Ensure that the body is in the format as shown in the screenshot below, otherwise the API testing won't work. </p> </li> <li> <p>In the Test response section, you need to provide a test response from the model:</p> <ul> <li> <p>Click the Test button under the Test section on the Custom API Integration dialog.    The Sample Input dialog is displayed. </p> </li> <li> <p>Enter a Prompt, Sample prompt, and Examples in the respective fields.</p> </li> </ul> </li> <li> <p>Once the response is generated after the Test, you can configure the JSON path to capture the Output path, Input tokens, and Output tokens.</p> <p>Note</p> <p>Click the Save as draft button to save the model and the status is saved as Draft.</p> </li> <li> <p>Click Confirm to save the details and your external model is listed in the External model's list. The model can now be used in the playground and the Gen AI node of the agent flow builder.</p> <p>Note</p> <p>You can click the 3 dots icon corresponding to the Model name in the list of external models and edit or delete the model.</p> <p></p> </li> </ol>"},{"location":"models/external-models/add-an-external-model-using-easy-integration/","title":"Add an External Model using Easy Integration","text":"<p>In this topic, you can see the process of adding the Claude-V1 model from the provider Anthropic.</p> <p>To add an external model using easy integration, follow these steps:</p> <ol> <li>Click Models on the top navigation bar of the application. The Models page is displayed.</li> <li> <p>Click the External models tab on the Models page.</p> <p></p> </li> <li> <p>Click Add a model under the External models tab. The Add an external model dialog is displayed.</p> <p></p> </li> <li> <p>Select the Easy integration option to integrate models from Open AI, Anthropic, Google, or Cohere and click Next.</p> </li> <li> <p>Select a provider to integrate with and click Next.</p> <p></p> <p>A Pop-up with the list of all the Anthropic models that are supported in GALE is displayed.</p> <p></p> </li> <li> <p>Select the required Model and click Next.</p> </li> <li> <p>Enter the respective API key you have received from the provider in the API key field and click Confirm to start the integration.</p> <p></p> <p>The model is integrated and is listed in the External models list.</p> </li> </ol> <p>Note</p> <ul> <li>You can click the 3 dots icon corresponding to the Model name in the list of external models and edit or delete the model.</li> <li>You can set the Inference option using the toggle button corresponding to the Model name. If the Inferencing toggle is ON, you can use this model across GALE. If the toggle button is OFF, it means you cannot infer it anywhere in GALE. For example, if you turn OFF the toggle button, then in the playground, an error message is displayed that the model is not active even though you have added it in the external models tab.</li> </ul>"},{"location":"models/fine-tune-models/configure-your-fine-tuned-model/","title":"Configure your Fine-Tuned Model","text":"<p>You can modify the general details of your fine-tuned model if required.</p> <p>To modify the settings of your fine-tuned model, follow the steps:</p> <ol> <li> <p>Click the Configurations tab from the left panel on the Models page. The Configurations page is displayed.</p> <p></p> </li> <li> <p>Make any required changes and the changes are auto-saved.</p> </li> <li>You also have the option to suspend your deployed fine-tuned model using the Proceed to undeploy button. It immediately un-deploys the model and is not available for any inferencing requests.</li> </ol> <p>Note</p> <p>To delete a fine-tuned model, you must first un-deploy it and then delete it. </p>"},{"location":"models/fine-tune-models/create-a-fine-tuned-model/","title":"Create a Fine-Tuned Model","text":"<p>You can fine-tune a Kore-hosted model or import one from Hugging Face. The fine-tuning process involves the following steps:</p> <ol> <li>General details</li> <li>Selecting a base model</li> <li>Fine-tuning configuration</li> <li>Adding the training and evaluation datasets</li> <li>Adding the test dataset (optional)</li> <li>Selecting a hardware</li> <li>Integrating with Weights &amp; Biases (optional)</li> </ol> <p>To fine-tune a model, follow these steps:</p> <ol> <li> <p>Go to Models &gt; Fine-tuned models and click Start fine-tuning.</p> </li> <li> <p>The Create a fine-tuned model dialog is displayed. In the General details section:</p> <ul> <li> <p>Enter a Model name and Description for the fine-tuned model. </p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> </li> <li> <p>In the Base model section, choose the model to be fine-tuned.</p> <ul> <li> <p>If you choose Kore-hosted models, select the model from the dropdown menu and click Next.   </p> </li> <li> <p>If you choose to Import from Hugging Face, select the Hugging Face connection type from the dropdown, and paste the model name. Click Next. For more information about how to connect to your Hugging Face account, see How to Connect to your Hugging Face Account.  </p> </li> </ul> </li> <li> <p>In the Fine-tuning configuration section:</p> <ul> <li>Select a Fine-tuning type, which you want to apply to the model. </li> <li>Enter the Number of Epochs, which indicates how many times the model processes the entire dataset during training.</li> <li>Enter a number for Batch size, which implies the number of training examples used in one training iteration.</li> <li>Enter a value for the Learning rate, which implies the size of the steps taken during the optimization of a model.</li> <li>Click Next. </li> </ul> </li> <li> <p>In the Dataset section:</p> <ul> <li> <p>Select or upload the Training dataset from the dropdown to train the base model. It acts as the foundation for the model's learning.</p> <p>Note</p> <p>The system accepts JSONL, CSV, and JSON files. The training, evaluation, and test files must follow a specific format with at least two columns: one for the prompt and one for the completion. You can download a sample file. </p> <p></p> </li> <li> <p>Evaluation dataset: Select the dataset for the model evaluation and then click Next.</p> <ol> <li>Use from training dataset (default): This enables you to allocate a percentage of the training dataset for model evaluation. By default, 15% of the training dataset is allocated for model evaluation.</li> <li>Upload evaluation dataset: Select or upload another dataset from the dropdown.</li> <li>Skip the evaluation: It will skip the model evaluation process. </li> </ol> </li> </ul> </li> <li> <p>Select or upload the test dataset to test the fine-tuned model. Click Next.     </p> <p>Note</p> <p>The system accepts JSONL, CSV, and JSON files. </p> </li> <li> <p>Select the required hardware for fine-tuning from the dropdown menu and click Next.         </p> </li> <li> <p>In the Weights &amp; Biases section, select your WandB connection from the drop-down list and click Next.  To create a Weight &amp; Biases connection, click + New connection. For more information about how to create the WandB account, see How to Integrate with WandB </p> <p>Note</p> <p>You need an account with Weights and Biases. Enabling the integration with an API token will share your real-time fine-tuning status with the platform, allowing you to monitor your model's fine-tuning metrics comprehensively. Use the provided API token to create an integration, sending all fine-tuning process data to the associated account.</p> <p> </p> </li> <li> <p>In the Review step, verify all the details before starting the fine-tuning. To modify previous steps, click Back.      </p> </li> <li> <p>Click Start fine-tuning. The model Overview page displays real-time progress. You can also view the model\u2019s overview page by clicking the model on the Fine-tuned models page. Learn more. </p> </li> </ol> <p>Once testing is completed, you can download the training file, test results, and test data for your reference.</p> <p>After fine-tuning, deploy the model in GALE or externally via the generated API endpoint. You can also create another fine-tuned model on top of this one.</p>"},{"location":"models/fine-tune-models/create-a-fine-tuned-model/#create-a-fine-tuned-model-old-doc","title":"Create a Fine-Tuned Model- Old Doc","text":"<p>You can create a fine-tuned model in the Create a fine-tune model wizard, which involves the following 5 steps:</p> <ul> <li>General details</li> <li>Selecting a base model</li> <li>Fine-tuning configuration</li> <li>Adding the training and evaluation datasets</li> <li>Adding the test dataset (optional)</li> <li>Selecting a hardware</li> <li>Integrating with Weights &amp; Biases (optional)</li> </ul> <p>Let us now look into each step in detail.</p> <p>To create a fine-tuned model, follow these steps:</p> <ol> <li> <p>Click Models on the top navigation bar of the application. The Models page is displayed.</p> <p></p> </li> <li> <p>Click the Create a fine-tuned model button on the Models page. The Create a fine-tuned model dialog is displayed.</p> </li> <li> <p>In the General details section:</p> <ul> <li> <p>Enter a Model name and Description for your fine-tuned model.</p> <p></p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> <p>In the Base model section, do one of the following:</p> <ul> <li>Select form the Kore hosted models.</li> </ul> <p>Or</p> <ul> <li>Select Hugging Face connection to use. For more information about how to connect to Hugging Face account, see How to Connect to your Hugging Face Account.</li> </ul> <p></p> </li> <li> <p>In the Fine-tuning configuration section:</p> <ul> <li> <p>Select a Fine-tuning type that you want to fine-tune for your requirements.</p> </li> <li> <p>Enter the Number of Epochs which implies the number of times the entire data set is passed through the model during the training process.</p> <p></p> </li> <li> <p>Enter a number for Batch size which implies the number of training examples used in one iteration of training.</p> </li> <li> <p>Enter a value for the Learning rate which implies the size of the steps taken during the optimization of a model.</p> </li> <li> <p>Click Next to proceed to the next step.</p> </li> </ul> </li> <li> <p>In the Dataset section:</p> <ul> <li>Select the Training dataset which is the data that will be used in training the base model and acts as the foundation for the model's learning.</li> </ul> <p>Note</p> <p>The format accepted is JSONL, CSV, or JSON. The training file, evaluation file, and the test file follow a specific format. The file should have at least two columns with one column as prompt and other as completion. In the wizard, you have an option to download the sample as well.</p> <ul> <li> <p>Define the data for evaluation. You can use the same training data set in the evaluation process as well. In this step, you can either allocate a percentage of the training dataset to use for evaluation, or upload a new evaluation dataset, or skip the evaluation.</p> <p></p> </li> <li> <p>Click Next to proceed to the next step.</p> </li> </ul> </li> <li> <p>In the Test data section:</p> <ul> <li>Upload a dataset with which you want to test your fine-tuned model.</li> </ul> <p>Note</p> <p>The format accepted is JSONL, CSV, or JSON. </p> <p></p> <ul> <li>Click Next to proceed to the next step.</li> </ul> <p>Note</p> <p>If you change the training file or the evaluation file in the Dataset section of the wizard, then a warning is displayed in the Test data section. You can verify and proceed.</p> <p>In the Hardware section:</p> <ul> <li> <p>Select the hardware required for fine-tuning your model.</p> <p></p> </li> </ul> </li> <li> <p>In the Weights &amp; Biases section:</p> <ul> <li> <p>Select your WandB connection from the drop-down list. If you don't have a WandB connection, you can click the + New connection that is available in the drop-down list. For more information about how to create the WandB account, see How-to-Integrate-with-WandB.</p> <p>Note</p> <p>You need an account with Weights and Biases. Enabling the integration with the help of an API token will share your real-time fine-tuning status with the platform and you can comprehensively monitor the fine-tuning metrics of your model. You can use the API token they provided to create an integration and then all the data related to the fine-tuning process will be sent to the account related to that API token.</p> <p></p> </li> <li> <p>Click Next to proceed to the next step.</p> </li> </ul> </li> <li> <p>In the Review step, verify all the details that you provided earlier.</p> <p>Note</p> <p>If you want to make any modifications, you can go to the previous step by clicking the Back button or a particular step indicator on the left panel.</p> <p></p> </li> <li> <p>Click Start fine-tuning. The Overview page of your fine-tuned model is displayed with the status \u201cInitializing\u201d.</p> <p></p> <p>The different statuses that are involved in the process include:</p> <ul> <li> <p>Initializing</p> </li> <li> <p>Training in progress</p> </li> <li> <p>Testing in progress</p> </li> <li> <p>Fine-tuning completed</p> </li> </ul> <p>Note</p> <p>You can stop the process if required in the middle and resume it later, then the status is changed to \u201cStopped\u201d. Then a Re-trigger button is displayed to initiate the fine-tuning process again from the beginning. If the fine-tuning process fails, then the status is changed to \u201cFailed\u201d and you can view the reason for failure. You can make the required changes, and click the Re-trigger button to start the fine-tuning again.</p> <p>Once the fine-tuning is triggered, you can check the progress in real-time on the Overview page. You can click any model name from the list of fine-tuned models and the Overview page of the model is displayed.</p> <p>This page displays the following sections:</p> <ul> <li> <p>Training information: This section captures crucial information such as the type of training being performed, the step at which the training is currently, training duration, training loss, and validation loss, graphically displays them with a click of arrows, and plots the overall loss trend. You can click the arrow keys to access the graph.</p> </li> <li> <p>Test data information: This section indicates how well your fine-tuned model performed on the test dataset (if you provided any) with the help of a metric called the BLEU score.</p> </li> <li> <p>Training parameters: This section displays the summary of the parameters you have provided in the wizard and it is displayed for your convenience.</p> </li> <li> <p>System information: This section captures the information about the Hardware- CPUs and GPUs functioning at the backend and its utilization during the fine-tuning process.</p> </li> </ul> </li> </ol> <p>You can also download the training file for your reference from this page. You also have the option to download the test result and the test data from the test info section once the testing is completed.</p> <p>Note</p> <p>If you want to perform the testing again, you can click the Retry option corresponding to the Test info section on the Overview page and select a new test data set file or use the existing file and confirm to start the testing again. The status then is displayed as \u201cTesting in Progress\u201d.</p> <p></p> <p>After the model is fine-tuned, you can deploy the fine-tuned model and use it across GALE and also externally via the API endpoint that is generated after deploying the model.</p> <p>Note</p> <p>When fine-tuning is completed, you can use it to create another fine-tuned model on top of this model.</p>"},{"location":"models/fine-tune-models/deploy-a-fine-tuned-model/","title":"Deploy a fine-tuned model","text":"<p>Once the fine-tuning process is completed, you can deploy your fine-tuned model.</p> <p>To deploy your fine-tuned model, follow these steps:</p> <ol> <li> <p>Do one of the following:</p> <ul> <li> <p>Click the Deploy model button on the Overview page of your fine-tune model.</p> <p></p> </li> </ul> <p>Or</p> <ul> <li> <p>Click the Model Endpoint from the left panel on the of your fine-tune model and then click Deploy model button.</p> <p></p> </li> </ul> </li> <li> <p>Click Deploy model. The Deploy dialog is displayed.</p> </li> <li> <p>In the General details section:</p> <ul> <li> <p>Enter a Deployment name and Description for your model.</p> <p></p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> </li> <li> <p>In the Parameters section:</p> <ul> <li> <p>Select the Sampling Temperature to use for deployment.</p> </li> <li> <p>Select the Maximum length which implies the maximum number of tokens to generate.</p> </li> <li> <p>Select the Top p which is an alternative to sampling with temperature where the model considers the results of the tokens with top_p probability mass.</p> </li> <li> <p>Select the Top k value which is the number of highest probability vocabulary tokens to keep for top-k-filtering.</p> </li> <li> <p>Enter the Stop sequences which implies that where the model will stop generating further tokens.</p> </li> <li> <p>Enter the Inference batch size which is used to batch the concurrent requests at the time of model inferencing.</p> </li> <li> <p>Select the Min replicas which is the minimum number of model replicas to be deployed.</p> </li> <li> <p>Select the Max replicas which is the maximum number of model replicas to auto-scale.</p> </li> <li> <p>Select the Scale up delay (in seconds) which is how long to wait before scaling-up replicas.</p> </li> <li> <p>Select the Scale down replicas (in seconds) which is how long to wait before scaling down replicas.</p> <p></p> </li> </ul> </li> <li> <p>Click Next.</p> </li> <li> <p>In the Hardware section:</p> <ul> <li> <p>Select the Hardware name required for the deployment and click Next.</p> <p></p> </li> <li> <p>In the Review step, verify all the details that you provided earlier. Select the I accept all the terms and conditions check box.</p> <p></p> </li> </ul> <p>Note</p> <p>If you want to make any modifications, you can go to the previous step by clicking the Back button or a particular step indicator on the left panel.</p> </li> <li> <p>Click Deploy.</p> <p>After the deployment process is complete the status is changed to \u201cDeployed\u201d. You can now infer this model across GALE and externally. The deployment of your model will start and after the deployment process is complete, you can find the API endpoint created for your fine-tuned model.</p> </li> </ol>"},{"location":"models/fine-tune-models/export-your-fine-tuned-model/","title":"Export your Fine-Tuned Model","text":"<p>You can export your fine-tuned model for reference.</p> <p>To export your fine-tuned model, follow these steps:</p> <ol> <li> <p>Click the three dots icon corresponding to the Model name on the Models page. A pop-up with a list of options is displayed.</p> <p></p> </li> <li> <p>Click Export model from the list of options. The exported zip file is saved in your downloads folder.</p> </li> </ol>"},{"location":"models/fine-tune-models/generate-an-api-key/","title":"Generate a New API Key","text":"<p>You can generate an API for your fine-tuned model and share it with other trusted users. It is essential to have a secure API key when trying to connect to this fine-tuned model in an external environment.  </p> <p>To generate a new API key for your fine-tuned model, follow these steps:</p> <ol> <li> <p>Click the API keys tab in the left panel on the Overview page of your fine-tuned model.</p> <p></p> </li> <li> <p>Click the Create a new API key button. The Create new API key dialog is displayed.</p> <p></p> </li> <li> <p>Enter a Name for the key and click the Generate key button. Your API key is now generated. Click Copy and close button to copy your API key and share it with others if required. </p> <p></p> </li> </ol>"},{"location":"models/fine-tune-models/model-settings-overview/","title":"Overview","text":"<p>The model overview page displays real-time progress of the Fine-tuned models. </p> <p>To access the model\u2019s overview page,  go to  Models &gt; Fine-tuned models and click Model. </p> <ul> <li> <p>General Information: This section displays the model's progress status, Total time, and Author details.  </p> <ul> <li> <p>The progress statuses include Initializing, Training in progress, Testing in progress, Fine-tuning completed, Stopped, and Failed.</p> <p>Note</p> <p>If fine-tuning fails, the status changes to \"Failed.\" View the reason, make necessary changes, and click the Re-trigger to restart fine-tuning.</p> </li> <li> <p>If necessary, you can stop the process in the middle and resume it later; then, the status is changed to \u201cStopped.\u201d Click Re-trigger to initiate the fine-tuning process again from the beginning.</p> </li> </ul> </li> <li> <p>Base model information: This section shows the base model used for fine-tuning and its source.</p> </li> <li> <p>Training information: This shows important details like training type, number of training steps, training loss, percentage of validation data, validation loss, training start time, and duration. Click the downward arrows next to the training loss and validation loss fields to see their graphical trends.</p> </li> <li> <p>Test data information: It indicates the model performance on the test dataset using the BLEU score.</p> </li> <li> <p>Hardware information: This displays CPU and GPU utilization during fine-tuning.</p> </li> <li> <p>Training parameters: Displays the summary of the parameters provided.</p> </li> </ul>"},{"location":"models/fine-tune-models/view-the-generated-api-endpoint/","title":"View the Generated API Endpoint","text":"<p>After the model is deployed, the API endpoint is generated which implies that your fine-tuned model is ready for inferencing externally and across the other sections in GALE.</p> <p>Note</p> <p>You will receive an email notification after your model deployment is completed and an API is generated and ready to use.</p> <p>To view the API Endpoint, follow these steps:</p> <ol> <li> <p>Click the Model Endpoint tab in the left panel on the Models page of your fine-tuned model.</p> </li> <li> <p>Click the Copy icon to copy and share the API Endpoint.</p> <p></p> </li> </ol> <p>The API endpoint is available in 3 formats. You can copy and use the same as required.</p>"},{"location":"models/fine-tune-models/view-the-generated-api-endpoint/#api-endpoint-usecase","title":"API Endpoint Usecase","text":"<p>You can use the deployed fine-tuned model in GALE for the following use cases:</p> <ul> <li> <p>In the Prompt Playground to compare prompts against commercial, open-source, or any other fine-tune model.</p> </li> <li> <p>In an agent in the agent flow builder via the Gen AI Node.</p> </li> </ul>"},{"location":"models/open-source-models/configure-your-open-source-model/","title":"Configure your Open-Source Model","text":"<p>You can modify the general details of the open-source model if required.</p> <p>To modify the settings of the open-source model, follow the steps:</p> <ol> <li> <p>Click the Configurations tab from the left panel on the Models page. The Configurations page is displayed.</p> <p></p> </li> <li> <p>Make any required changes in the Description, or Tags fields, and the changes are auto-saved.</p> </li> <li>You also have the option to suspend your deployed custom model using the Proceed to undeploy button. It immediately un-deploys the model and is not available for any inferencing requests. Then the status of the model changes to \u201cReady to deploy\u201d. You can deploy the model again from the Deploy section to use it.</li> </ol> <p>Note</p> <p>To delete an open-source model, you must undeploy it and then only you can delete it.</p>"},{"location":"models/open-source-models/deploy-an-imported-model-from-hugging-face/","title":"Deploy an imported model from Hugging Face","text":"<p>You can deploy an open-source model by selecting the Hugging Face option in the deployment process.</p> <p>To deploy a model from Hugging Face, follow these steps:</p> <ol> <li>Click Models on the top navigation bar of the application. The Models page is displayed.</li> <li> <p>Click the Open-source models tab on the Models page.</p> <p></p> </li> <li> <p>Click the Deploy a model button. A pop-up with a list of available models is displayed.</p> <p></p> </li> <li> <p>Click the Hugging Face option from the list. The Hugging Face dialog is displayed.</p> </li> <li> <p>In the General details section:</p> <ul> <li> <p>Enter a Deployment name and Description for your model.</p> <p></p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> </li> <li> <p>In the Import model section:</p> <ul> <li> <p>Select the Hugging Face connection to use from the drop-down list. For more information about How to enable Hugging Face in GALE, see How to add a connection with Hugging Face.</p> <p></p> </li> <li> <p>Enter the Hugging Face model name from Hugging Face that you wish to import and click Next.</p> </li> </ul> </li> <li> <p>In the Parameters section:</p> <ul> <li> <p>Select the Sampling Temperature to use for deployment.</p> </li> <li> <p>Select the Maximum length which implies the maximum number of tokens to generate.</p> </li> <li> <p>Select the Top p which is an alternative to sampling with temperature where the model considers the results of the tokens with top_p probability mass.</p> </li> <li> <p>Select the Top k value which is the number of highest probability vocabulary tokens to keep for top-k-filtering.</p> </li> <li> <p>Enter the Stop sequences which implies that where the model will stop generating further tokens.</p> </li> <li> <p>Enter the Inference batch size which is used to batch the concurrent requests at the time of model inferencing.</p> </li> <li> <p>Select the Min replicas which is the minimum number of model replicas to be deployed.</p> </li> <li> <p>Select the Max replicas which is the maximum number of model replicas to auto-scale.</p> </li> <li> <p>Select the Scale up delay (in seconds) which is how long to wait before scaling-up replicas.</p> </li> <li> <p>Select the Scale down replicas (in seconds) which is how long to wait before scaling down replicas.</p> <p></p> </li> </ul> </li> <li> <p>Click Next.</p> </li> <li> <p>In the Hardware section:</p> <ul> <li> <p>Select the Hardware name required for the deployment and click Next.</p> <p></p> </li> <li> <p>In the Review step, verify all the details that you provided earlier. Select the I accept all the terms and conditions check box.</p> <p></p> </li> </ul> <p>Note</p> <p>If you want to make any modifications, you can go to the previous step by clicking the Back button or a particular step indicator on the left panel.</p> </li> <li> <p>Click Deploy.</p> </li> </ol>"},{"location":"models/open-source-models/generate-an-api-key-open-source/","title":"Generate a New API Key","text":"<p>You can generate an API key for your open-source model and share it with other trusted users. It is essential to have a secure API key when trying to connect to this open-source model in an external ecosystem.  </p> <p>To generate a new API key for your open-source model, follow these steps:</p> <ol> <li> <p>Click the API keys tab from the left panel on the Models page.</p> <p></p> </li> <li> <p>Click the Create a new API key button. The Create new API key dialog is displayed.</p> <p></p> </li> <li> <p>Enter a Name for the key and click the Generate key button. Click the Copy and close button to copy your API key and share it with others if required.</p> <p></p> <p>All the generated API keys are listed in the API key section. You can hover over any key and find the delete icon corresponding to the name of the key if you want to delete the key.</p> </li> </ol>"},{"location":"models/open-source-models/select-and-deploy-an-open-source-model/","title":"Select and Deploy an Open-Source Model","text":"<p>GALE supports open-source models and provide them as a service.</p> <p>To select and deploy a model, follow these steps:</p> <ol> <li> <p>Click Models on the top navigation bar of the application. The Models page is displayed.</p> </li> <li> <p>Click the Open-source models tab on the Models page. </p> </li> <li> <p>Click the Deploy a model button. A pop-up with a list of available models is displayed. </p> </li> <li> <p>Select and click a model from the list. You can select Hugging Face also from the list of options. For more information about Hugging Face, see Deploy an Imported Model from Hugging Face.    </p> </li> <li> <p>The Deploy dialog is displayed. In the General details section:</p> <ul> <li> <p>Enter a Deployment name and Description for your model. </p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> </li> <li> <p>In the Parameters section:</p> <ul> <li> <p>Select the Sampling Temperature to use for deployment.</p> </li> <li> <p>Select the Maximum length which implies the maximum number of tokens to generate.</p> </li> <li> <p>Select the Top p which is an alternative to sampling with temperature where the model considers the results of the tokens with top_p probability mass.</p> </li> <li> <p>Select the Top k value which is the number of highest probability vocabulary tokens to keep for top-k-filtering.</p> </li> <li> <p>Enter the Stop sequences which implies that where the model will stop generating further tokens.</p> </li> <li> <p>Enter the Inference batch size which is used to batch the concurrent requests at the time of model inferencing.</p> </li> <li> <p>Select the Min replicas which is the minimum number of model replicas to be deployed.</p> </li> <li> <p>Select the Max replicas which is the maximum number of model replicas to auto-scale.</p> </li> <li> <p>Select the Scale up delay (in seconds) which is how long to wait before scaling-up replicas.</p> </li> <li> <p>Select the Scale down replicas (in seconds) which is how long to wait before scaling down replicas. </p> </li> </ul> </li> <li> <p>Click Next.</p> </li> <li> <p>In the Hardware section:</p> <ul> <li>Select the Hardware name required for the deployment and click Next. </li> </ul> </li> <li> <p>In the Review step, verify all the details that you provided earlier. Select the I accept all the terms and conditions check box. </p> <p>Note</p> <p>If you want to make any modifications, you can go to the previous step by clicking the Back button or a particular step indicator on the left panel.</p> </li> <li> <p>Click Deploy.</p> <p>Note</p> <p>You will be charged for deployment and inferencing-related costs for each open-source model.</p> </li> </ol> <p>After the deployment process is complete the status is changed to \u201cDeployed\u201d. You can now use this model across GALE and externally.</p> <p>Click the three dots icon corresponding to the Model name under the Open-source models tab on the Models page. A pop-up with a list of options is displayed. If you choose the API endpoint option, it will navigate you to the API keys section of the Model and if you select the Configurations option, it will take you to the Configurations section of the Model.</p> <p></p>"},{"location":"models/open-source-models/select-and-deploy-an-open-source-model/#re-deploy-a-deployed-model","title":"Re-deploy a Deployed Model","text":"<p>After initially deploying a model, if you want you can modify some parameter values and redeploy the updated model.</p> <p>Note</p> <ul> <li> <p>To re-deploy a model you must first undeploy the existing model and then deploy the updated one. This means there will be some downtime when the model is unavailable.</p> </li> <li> <p>The model's endpoint remains the same even after undeploying and redeploying the model.</p> </li> </ul> <p>To re-deploy a deployed model, follow these steps:</p> <ol> <li>Click the required model name from the Open-source models list.</li> <li>Click the Deploy model button. The Model Configuration page is displayed.</li> <li>Modify the required fields and click the Deploy button. The model is deployed and status is \u201cDeployed\u201d.</li> </ol>"},{"location":"models/open-source-models/view-the-generated-api-endpoint/","title":"View the Generated API Endpoint","text":"<p>After the open-source model is deployed, the API endpoint is generated which implies that your model is ready for inferencing externally and across the other sections in GALE. </p> <p>The API endpoint is available in 3 formats.</p> <p>Note</p> <p>You will receive an email notification after your model deployment is completed and an API is generated, and it is ready to use.</p> <p>To view the API Endpoint, follow these steps:</p> <ol> <li> <p>Click the required model from the models listing. Click the Model Endpoint tab in the left panel on the Models page of your open-source model. The API endpoint created for this open-source model is displayed.</p> </li> <li> <p>To use this model as a service, the generated code is helpful. Click the Copy icon to copy and share the API Endpoint.</p> <p></p> </li> </ol> <p>Note</p> <p>Click the Deployment history tab on the Deploy page to view the history. This can be particularly useful for auditing and accountability purpose.</p> <p>You can either embed the curl or the code that is generated into your own applications or use it externally. </p>"},{"location":"playground/add-a-model/","title":"Add a Model","text":"<p>Once you have given your prompts in the Prompt box, you can test your prompts across different AI model(s) and decide on the required model.</p> <p>To add a model, follow these steps:</p> <ol> <li>Click the required Experiment from the Experiments page. The Prompt experiments page is displayed.</li> <li>Click Add Model in the model response section. A pop-up with a list of options is displayed.</li> <li> <p>Click Manage models from the list of options.</p> <p></p> <p>You will be redirected to the External models page to add a model. For more information about how to add an external model, see Add an external model.</p> <p>Note</p> <p>Follow the preceding process of adding an external model only if you are adding any model for the first time. If you have already added a model, it is automatically displayed in the pop-up list when you click the Add model button. For example, if you have added the Claude-v1 model from Anthropic then it is displayed in the pop-up and you click to select the model as shown in the following image.</p> <p></p> <p>You can add other models also to compare and decide which model provides the suitable output for your prompt. </p> </li> <li> <p>In the added model section click the + icon corresponding to the model\u2019s name.\u00a0 A pop-up with a list of options is displayed.</p> <p></p> </li> <li> <p>Repeat step 3 to add another model to compare the outputs. </p> </li> <li>Click the Model setting icon corresponding to the model\u2019s name to make any changes in the settings of the base model you have selected. </li> <li> <p>Click Generate Output after you have added the required number of models to compare. The Generated output for comparison is displayed in the Model section of the Prompt playground space for your reference as shown in the following image.</p> <p></p> </li> </ol> <p>Note</p> <p>You can click the response to view the expanded view of the response, click the copy icon corresponding to the response to copy and use the response, make any settings changes in the model to see a difference in the model response, and remove the model if required.</p>"},{"location":"playground/create-a-new-prompt-experiment/","title":"Create a New Prompt Experiment","text":"<p>A prompt experiment is a process of testing and comparing the performance of different AI models using a specific input called a prompt. The prompt can be a phrase, a question, or a paragraph of data, and the AI model generates an output based on the input.</p> <p>In a prompt experiment, you can create experiments in either a simple or an advanced mode. In the simple mode, you can easily input prompts and select from configured models to automatically generate outputs. In an advanced mode, you have more control over the experiment, allowing you to link your own data set, add few shot examples, and system prompts, to get the optimized results. </p> <p>Prompt experiments are useful for testing and comparing AI models for specific use cases and applications. </p> <p>To create a prompt experiment, follow these steps:</p> <ol> <li> <p>Click Playground on the top navigation bar of the application. The Experiments page is displayed.</p> <p></p> </li> <li> <p>Click the Create experiment button. The New experiment dialog is displayed.</p> <p></p> </li> <li> <p>Enter a name for your experiment in the Experiment name field.</p> </li> <li>Select the Type of experiment if it is Text-based, then click the Text generation option.</li> <li> <p>Click Create to start with your prompt experiment process. A blank Prompt experiment page is displayed where you can enter your prompt and add models to compare the generated outputs.</p> <p></p> </li> </ol>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/","title":"Explore Other Actions to Perform in the Prompt Playground","text":"<p>You can also perform other actions on the prompt page and use it for your reference. You can view the history of your experiment and restore the version of the prompt you like, export prompt data, and copy the experiment as a prompt or as an Gen AI node.</p> <p></p>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#copy-a-prompt","title":"Copy a Prompt","text":"<p>You can copy a prompt experiment and use it for your reference for easy access to the other sections in the product.</p> <p>You have 2 options to copy your prompt experiment:</p> <ul> <li>Copy as a Prompt: If you select the copy as a prompt option, then the user prompt, the few shot examples, and the system prompt are all copied. </li> <li>Copy as Gen AI node: If you select the copy as Gen AI node option, then along with the user prompt, the few shot examples, and the system prompt, the bookmarked model and the settings applied to the model are also copied. Once it is copied you can paste it as an Gen AI node in the Flow builder canvas. </li> </ul> <p>To copy a prompt, follow these steps:</p> <ul> <li> <p>On the Prompts experiment page, click Copy on the header of the page. A pop-up with options is displayed.</p> <p></p> </li> <li> <p>Click as prompt or as Gen AI node as per your requirement and the experiment is copied.</p> </li> </ul>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#restore-a-prompt-version-using-the-history","title":"Restore a Prompt Version using the History","text":"<p>You can review the timeline of prompts and their corresponding generated outputs, complete with date and time stamps. Additionally, you can restore a prompt from this section, enabling you to utilize it and implement any necessary modifications, thereby saving considerable time compared to creating a new prompt from scratch.</p> <p>To restore a prompt, follow these steps:</p> <ul> <li> <p>On the Prompts experiment page, click the History icon on the header of the page.</p> <p></p> <p>The history section is displayed with the list of all the old prompt experiments you created.</p> </li> <li> <p>Click the required old version of the experiment. The Restore option is displayed on the footer of the prompt experiment page.</p> <p></p> </li> <li> <p>Click Restore to work on the selected version of the prompt.</p> </li> </ul>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#export-prompt-data","title":"Export Prompt Data","text":"<p>You can download the prompt and the related experiment data and export it as a CSV file which can be used for later reference.\u00a0</p> <p>To export a CSV file, follow these steps:</p> <ul> <li>On the Prompts experiment page, click the download icon on the header of the page.</li> </ul> <p>The prompt experiment is downloaded as a CSV file successfully and is available in the downloads section of your computer.</p>"},{"location":"playground/generate-output-using-advanced-mode/","title":"Generate Output using Advanced Mode","text":"<p>While using the prompt playground, you have to upload a dataset in advanced mode as input. You can add and pre-view the existing or newly uploaded datasets. You can also replace one dataset with another based on the requirement for a particular prompt and generate multiple outputs simultaneously. </p> <p>The advanced mode option helps you improve the prompt creation and testing process more easily and efficiently. You can define a prompt to send to the models, add a few examples for the model to understand the output you are expecting, and upload a data set with inputs. In this mode, you can tweak and refine your prompts, generate the outputs for the selected models, and review the generated responses.\u00a0\u00a0\u00a0</p> <p>On the Prompt page, you can find a Prompt sample auto-populated based on the template you have selected. This is additional information/instruction given to the LLM while it processes the prompt. You can modify this prompt if required.</p> <p>To generate an output using advanced mode, follow these steps:</p> <ol> <li> <p>On the Prompt page, click the Advance Mode toggle button to shift to the advanced mode from the simple mode of output generation.</p> <p></p> <p>The Prompt playground view is enhanced for you with other options to explore.</p> <p></p> </li> <li> <p>Click the +Add Dataset link under the Dataset section on the Prompt page to upload your CSV file with data.</p> <p></p> <p>The Dataset dialog is displayed.</p> </li> <li> <p>Click Upload file to upload a CSV file from your local computer and click Upload. You can see a preview of all the data in your CSV file. Click Proceed to accept the file.</p> </li> <li> <p>Click the arrow under the Examples section on the Prompt section to add a sample user input and sample AI response output.</p> <p></p> </li> <li> <p>Click Save to save the sample examples.</p> </li> <li> <p>Click the down arrow corresponding to the Input field. A pop-up with a list of numbers such as 5, 10, 15, and 20 are visible. This displays the number of rows you want to see in your generated output from the CSV file that you have selected. Select the Randomize check box if you want to select randomly otherwise the list is sequential.</p> </li> <li> <p>The value for the variables in the {{}} braces is populated from the uploaded CSV file. You can add variables to the prompt by opening the brackets. For example, {{ xyz }} is a variable in the prompt box.\u00a0</p> <p>The value of the variable is populated from the dataset or file you have uploaded. To make sure the value is mapped correctly between the variable and the file, for the data to flow, you need to make the column name a variable within those curly braces.</p> <p></p> <p>For example,\u00a0{{Name}} is a variable, from the file you uploaded where Name is a column in that CSV file. A maximum of 20 names or rows that you select can flow from the file to the prompt input column of the advanced mode.</p> </li> <li> <p>Click the + icon corresponding to the Add Model section to add the models you want to select and compare the generated outputs.</p> <p>Note</p> <p>Currently, you can add only a maximum of 3 models in the playground. For more information about how to add a model to the prompt, see Add an external model.</p> </li> <li> <p>Click Generate Output after you have added the required number of models to compare. The Generated output for comparison is displayed in the Model section of the Prompt playground space for your reference.</p> <p>Note</p> <p>You can view the matrix in the generated output section which helps in deciding which prompt and model are better for your requirement.</p> <p></p> </li> </ol> <p>It displays the total tokens metric which implies how many tokens are sent to the model and how many are sent back in the response, the time taken by the model to respond with the answer, and the response and request in the JSON format. You can click the View in JSON format icon at the bottom of the generated output section to view the request and response generated in JSON format in a separate dialog box as shown in the following image.</p> <p></p> <p>If you want to view all the details of the outputs across all the models you can turn ON the toggle button by clicking the info icon on the Models section of the Playground space as shown in the following image. If you turn OFF the toggle button, then the metrics is not displayed.</p> <p></p> <p>You can collect feedback on the model's responses to refine its performance. Feedback is collected through simple Thumbs Up or Down on responses in the playground. Giving feedback is optional and does not hinder generating more responses. Thumbs up is for good responses. Thumbs down is for incorrect, hypothetical, or disliked responses. When you click Thumbs down icon a pop-up is displayed and you can add additional feedback if required.</p> <p>Note</p> <p>This feedback option is only applicable for fine-tuned and open-source models.</p> <p></p>"},{"location":"playground/overview/","title":"About Playground","text":"<p>GALE's prompt Playground allows you to experiment and refine prompts to get the best performance from LLMs. You can test different prompts across various models - external, fine-tuned, or open-source. The Playground helps you identify each prompt's ideal model and configurations through an iterative process. This streamlined workspace enables you to optimize prompts rapidly for maximum model effectiveness.</p>"},{"location":"playground/overview/#key-features","title":"Key Features","text":"<ul> <li> <p>Create a Prompt Experiment: Create experiments by selecting a template or writing a custom prompt and adding examples. Link a dataset, add one or more LLMs to generate responses, and compare the results. Learn more.</p> </li> <li> <p>Simple vs. Advanced Mode: Simple mode allows quickly generating output by adding a prompt and model. Advanced mode provides more control, enabling the addition of a prompt, system prompt, and examples, linking a dataset, and adding multiple models to generate and compare outputs. Learn more.  </p> </li> <li> <p>Resue the Prompt Experiment: Copy the prompt as a Gen AI node directly in the Agent Flow builder or copy it for use elsewhere. You can also export/download the prompt and related data in CSV format. Learn more.</p> </li> <li> <p>Restore a Prompt Experiment from History: Review the timeline of prompts and their corresponding generated outputs, and restore a prompt version if needed. Learn more.</p> </li> </ul>"},{"location":"playground/select-a-template-for-a-prompt/","title":"Select a Template for a Prompt","text":"<p>You can use the pre-defined templates available in GALE while providing prompts to make your experience easy and save time. There are different categories of templates that you can use to get the desired output. The different categories include analyzing documents, writing and conversation, learning, and coding. In each of the categories, there are a few templates that can be used for a specific task.\u00a0</p> <p>For example, if you select the \u201cWriting a blog post\u201d template from the Writing and Conversations category then the sample prompt is ready in the Prompt box which looks like this: \u201cGenerate a blog post on the topic {{topic}} in a {{ tone }} tone\u201d. Similarly, you can use the \u201cTeach me about GANs\u201d template from the Learning category, and the prompt will look like this: \u201cTeach me about the topic {{ topic name }} like I am a 5-year-old\u201d. You can provide the variables and generate the output for your reference. Variables are the values you provide in the curly braces when selecting a prompt template.</p> <p>Note</p> <p>In the simple mode, you input the required variable without the curly braces. In advanced mode, data is taken from the selected dataset file.\u00a0For more information about uploading a dataset, see Upload a dataset file.</p> <p>To select a template, follow these steps:</p> <ol> <li> <p>On the Prompts experiment page, click Templates corresponding to the Prompt box.</p> <p></p> <p>The list of templates is displayed.</p> </li> <li> <p>Click the required template from the category you require. For example, you can take a template \u201cText to bullet points\u201d and then the prompt is displayed in the Prompt box.</p> <p></p> </li> <li> <p>Enter a value for the variables by removing the {{ }} to get the desired output as shown in the following image.</p> <p>Note</p> <p>In the simple mode you must remove the {{}} and add the value in the prompt.</p> <p></p> <p>Once you generate an output, you will be able to view the output as shown in the following image.</p> <p></p> </li> </ol>"},{"location":"settings/integrations/enable-hugging-face/","title":"Enable Hugging Face","text":"<p>GALE seamlessly integrates with the Hugging Face platform, allowing you to incorporate cutting-edge text generation and text-to-text generation models. Any publicly available Hugging Face model can be swiftly deployed via GALE with minimal effort. To utilize private or exclusive Hugging Face models, you can effortlessly establish a connection by supplying your Hugging Face access tokens. This facilitates GALE in unleashing the complete capabilities of your Hugging Face account, regardless of whether you possess public or private assets.</p> <p>To integrate with your hugging face account, follow these steps:</p> <ol> <li> <p>Click the Settings icon on the top navigation bar of the application. The Integrations page is displayed.</p> <p></p> </li> <li> <p>Click the Hugging Face option from the list of Integrations. The Hugging Face section is expanded.</p> <p></p> </li> <li> <p>Click Add connection. The Hugging Face dialog is displayed.</p> <p></p> </li> <li> <p>Enter the following details in the dialog to create a connection:</p> <ul> <li>Provide a Connection name.</li> <li>Enter an Access token which is a unique identifier associated with your Hugging Face account.</li> </ul> </li> <li>Click Confirm to create a connection.</li> </ol>"},{"location":"settings/integrations/enable-hugging-face/#testing-your-connection-to-hugging-face","title":"Testing your connection to Hugging Face","text":"<p>You can test your connection after you provide the details to verify the accuracy of the details.</p> <p>To test your connection, follow these steps:</p> <ol> <li> <p>Click the Test button on the Hugging Face dialog.</p> <p></p> <p>Once the connection is tested, you will receive feedback information.</p> </li> <li> <p>If the connection is successful, you can click Confirm and complete the connection process.</p> </li> <li>If the connection fails, you can verify the details entered or cancel the set-up process.</li> <li> <p>You can test the connection by clicking the Play button on the connections list.</p> <p></p> <p>Note</p> <p>If the connection fails a red icon is displayed corresponding to the name of the connection on the Connections list.</p> </li> <li> <p>Hover over the connection name and click the three dots icon corresponding to the Connection name. The list of options is displayed. Click Edit to modify the connection details and Delete to delete the connection.</p> <p></p> <p>Note</p> <p>Once the Hugging Face connection is completed, you can see your connection name in the drop-down box while selecting and deploying an Open-source model.  For more information about selecting and deploying, see Select and Deploy an Open-Source Model</p> </li> </ol>"},{"location":"settings/integrations/integrate-with-s3-bucket/","title":"Integrate with S3 Bucket","text":"<p>The S3 Storage Integration functionality broadens GALE's capabilities by enabling connectivity with your AWS S3 account. It empowers you to import files from S3 and leverage them in developing high-quality AI applications for enterprises.</p> <p>To integrate with your S3 account, follow these steps:</p> <ol> <li> <p>Click the Settings icon on the top navigation bar of the application. The Integrations page is displayed.</p> <p></p> </li> <li> <p>Click the AWS S3 bucket option from the list of Integrations. The AWS S3 bucket section is expanded.</p> <p></p> </li> <li> <p>Click Add connection. The AWS S3 bucket dialog is displayed.</p> <p></p> </li> <li> <p>Enter the following details in the dialog to create a connection:</p> <ul> <li>Provide a Connection name.</li> <li>Enter an Access key which is a unique identifier associated with your AWS account.</li> <li>Enter a Secret key which is a confidential key paired with the Access Key ID.</li> <li>Enter a Bucket name which is configured in the S3 console.</li> </ul> </li> <li>Click Confirm to create a connection.</li> </ol>"},{"location":"settings/integrations/integrate-with-s3-bucket/#testing-your-connection-to-s3","title":"Testing your connection to S3","text":"<p>You can test your connection after you provide the details to verify the accuracy of the details.</p> <p>To test your connection, follow these steps:</p> <ol> <li> <p>Click the Test button on the AWS S3 bucket dialog.</p> <p>Once the connection is tested, you will receive feedback information.</p> </li> <li> <p>If the connection is successful, you can click Confirm and complete the connection process.</p> </li> <li>If the connection fails, you can verify the details entered or cancel the set-up process.</li> <li> <p>From the connections table, you can test the connection by clicking the Play button.</p> <p>Note</p> <p>If the connection fails a red icon is displayed corresponding to the name of the connection on the Connections list.</p> </li> </ol>"},{"location":"settings/integrations/integrate-with-s3-bucket/#how-to-use-files-from-the-connected-s3-buckets-in-the-agent-flow-builder","title":"How to use files from the connected S3 Buckets in the Agent Flow Builder","text":"<p>Once a connection is created and the integration of S3 Bucket is successful you can use the files in the Flow builder canvas.</p> <p>To use the files in the agents flow builder, follow these steps:</p> <ol> <li> <p>Create an Input variable with type Remote File. </p> </li> <li> <p>Add an API node and point to the remote file in the API node using the URL field. </p> <p>The file content will be available in the context object. You can access the file content in any other nodes using the context object.</p> </li> </ol>"},{"location":"settings/integrations/integrate-with-wandb/","title":"Integrate with Weights and Biases","text":"<p>Connecting with Weights and Biases (WandB) allows users to link to the platform, ensuring that the fine-tuning data associated with the model being refined in GALE is seamlessly transmitted to the WandB console for additional analytics.</p> <p>To integrate with your Weights and Biases (WandB) account, follow these steps:</p> <ol> <li> <p>Click the Settings icon on the top navigation bar of the application. The Integrations page is displayed.</p> <p></p> </li> <li> <p>Click the Weights &amp; Biases option from the list of Integrations. The Weights &amp; Biases section is expanded.</p> <p></p> </li> <li> <p>Click Add connection. The Weights &amp; Biases dialog is displayed.</p> <p></p> </li> <li> <p>Enter the following details in the dialog to create a connection:</p> <ul> <li>Provide a Connection name.</li> <li>Enter an API key which is a unique identifier associated with your WandB account.</li> </ul> </li> <li>Click Confirm to create a connection.</li> </ol>"},{"location":"settings/integrations/integrate-with-wandb/#testing-your-connection-to-weights-biases","title":"Testing your connection to Weights &amp; Biases","text":"<p>You can test your connection after you provide the details to verify the accuracy of the details.</p> <p>To test your connection, follow these steps:</p> <ol> <li> <p>Click the Test button on the Weights &amp; Biases dialog.</p> <p></p> <p>Once the connection is tested, you will receive feedback information.</p> </li> <li> <p>If the connection is successful, you can click Confirm and complete the connection process.</p> </li> <li>If the connection fails, you can verify the details entered or cancel the set-up process.</li> <li> <p>You can test the connection by clicking the Play button on the connections list.</p> <p></p> <p>Note</p> <p>If the connection fails a red icon is displayed corresponding to the name of the connection on the Connections list.</p> </li> <li> <p>Hover over the connection name and click the three dots icon corresponding to the Connection name. The list of options is displayed. Click Edit to modify the connection details and Delete to delete the connection.</p> <p></p> </li> </ol> <p>Note</p> <p>Once the WandB connection is completed, you can see your connection name in the drop-down box in the Create a fine-tuned model wizard in the Weights and Biases section.  For more information about the fine-tuning wizard, see Create a Fine-Tuned Model.</p>"},{"location":"settings/user-management/invite-a-user-to-your-account/","title":"Invite a User to your Account","text":"<p>As the owner of the account in GALE, you have the authority to invite any user and grant them access to your account for viewing and working in GALE. When you send an invitation to a user, they receive an invitation email through which they can access the product.</p> <p>To invite a user to your account, follow these steps:</p> <ol> <li> <p>Click the Profile icon from the top navigation bar of the product. A pop-up with a list of options is displayed.</p> <p></p> </li> <li> <p>Hover over Account name from the pop-up list and click the Invite user to the account option from the list as shown below.</p> </li> </ol> <p></p> <pre><code>The **Invite users to &amp;lt;&lt;account name&gt;&gt;** dialog is displayed.\n</code></pre> <ol> <li> <p>Enter the Email ID of the user to add to the account and click Invite.</p> <p></p> <p>An Email is sent to the invitee and the user can access the product now.</p> <p>Note</p> <p>Once the user clicks the Open Invite button from the email, the Sign-up page of GALE is displayed. The new user can add a Full name and Password and create a new account.</p> </li> </ol>"},{"location":"settings/user-management/overview/","title":"About User Management","text":"<p>User management within GALE operates on two distinct levels:</p> <ul> <li> <p>Account level: At the account level, administrators can manage users with a broader scope, encompassing the entire GALE account. This includes responsibilities such as user registration, access control, and permissions applicable across various applications within the GALE platform.</p> </li> <li> <p>Application level: User management becomes more detailed and specific at the application level, concentrating on individual applications within the GALE platform. The owner of each respective application holds the authority to extend invitations to any number of individual users, customizing their capabilities in alignment with the unique requirements of each application. For more information about setting app-level permissions, see Manage user roles and permissions.</p> </li> </ul>"}]}