{"config":{"lang":["tr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"core-capabilities-of-gale/","title":"Core capabilities of GALE","text":"<p>The GALE platform boasts a multitude of capabilities that address the rapid changes in the dynamic landscape of Language Models (LLMs). The ever-evolving nature of LLMs presents challenges, and GALE is a versatile toolkit to navigate and harness the opportunities arising from these swift transformations.</p> <p>The following are a few capabilities GALE has to offer:  * Data privacy: When you train your LLM, you have greater control over the data used in the training process. This is crucial for organizations dealing with sensitive information, as it mitigates the risks associated with sharing proprietary or confidential data with external models. Keeping the training process in-house allows you to adhere to strict privacy regulations and safeguard sensitive information.</p> <ul> <li> <p>Tailored to Domain-Specific Requirements: In many cases, off-the-shelf models may not fully address the intricacies of specific industries or domains. By training your LLM, you can incorporate domain-specific knowledge and nuances, ensuring the model is finely tuned to excel in tasks relevant to your organization's field.</p> </li> <li> <p>Cost-Effectiveness: While training your own LLM may involve initial resource investment, it is cost-effective in the long run. Avoiding recurring fees associated with external models and services may lead to significant cost savings over time, especially for organizations with sustained, high-volume usage.</p> </li> <li> <p>Alignment with Business Goals: By training your models, enterprises can gain greater control over the objectives, values, and behaviors of your systems. This enables a direct alignment with your business goals, ensuring that the models are finely tuned to meet specific organizational objectives. Through the training process, enterprises can shape and tune the models in a way that resonates with your business.</p> </li> <li> <p>Seamless Integration with Business Processes: Models trained on an enterprise's data, applications, and systems offer a level of seamless deployment and integration into business processes. The familiarity with internal data structures and processes makes the deployment and integration of these models more straightforward. This tight coupling ensures a smooth incorporation into existing workflows and interfaces.</p> </li> <li> <p>Unique Competitive Advantage: Creating proprietary Language Models (LLMs) through in-house training offers enterprises a distinctive competitive advantage that sets them apart in the market. These models developed exclusively with the organization's data, represent a unique intellectual asset that competitors may not possess. This level of customization can be a key factor in gaining dominance in the targeted markets.</p> </li> </ul>"},{"location":"introduction/","title":"Introduction","text":"<p>GALE, an acronym for Generative AI-Language Model platform, serves as a comprehensive platform designed for constructing applications powered by Language Model (LLM) technology. This versatile platform equips you with a suite of tools to seamlessly amalgamate essential components necessary for developing modern, customized AI applications tailored to diverse use cases within organizations.</p> <p>One of GALE's notable features is its capability to harness existing open-source or commercial models, enabling you to fine-tune these models to suit specific requirements. This adaptability empowers organizations to leverage AI advantages without necessitating substantial alterations to your existing systems or processes. GALE facilitates the integration of applications into current systems and workflows, ensuring a seamless incorporation of AI benefits.</p> <p>For example, consider a use case in claims processing where you want to employ an AI model capable of autonomously determining whether a particular claim is eligible for automatic payment. In this scenario, the AI model performs various steps, such as comparing the original insurance image with the submitted claim image. Based on predefined rules and regulations, the model generates a report indicating whether the claim can be processed and automatically paid.</p> <p>Applications developed through GALE offer tailored solutions for your specific tasks, such as deal summarization or the automated generation of emails, social media summarization, or agent conversation summarization and key point extraction. This platform empowers organizations to address unique challenges by leveraging the capabilities of language models, promoting efficiency and innovation in various domains.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2016-2023 Martin Donath</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"data/overview/","title":"Overview","text":"<p>The data set module streamlines data management by allowing you to store your datasets in the application instead of uploading them every time you test prompts in the playground or the models or while fine-tuning the model. Additionally, the module provides flexibility to use different file formats such as CSV, JSON, and JSONL without any additional formatting requirements, ultimately enhancing convenience for you while working with large datasets within the product.</p>"},{"location":"data/upload-a-dataset/","title":"Upload a dataset","text":"<p>You can upload a dataset of your choice in CSV, JSON, or, JSONL format.</p> <p>To upload a dataset, follow these steps:</p> <ol> <li> <p>Click Data on the top navigation bar of the application. The Data page is displayed.</p> <p></p> </li> <li> <p>Click the Upload dataset button to upload your data from the location. The uploaded file is listed on the Data page.</p> <p></p> </li> <li> <p>When you select the Advance mode in the Prompt Playground and click the Add dataset option you can view and select the dataset you have added from the list as shown in the following image.</p> <p></p> </li> </ol> <p>Note</p> <p>Click the three dots icon corresponding to the name of the dataset to delete the dataset if you no longer require it. However, if you are using a data file in the Playground section and delete the file from the Data page the Playground experiments are impacted, and an error message is displayed.</p>"},{"location":"models/overview/","title":"Overview","text":"<p>The Model Studio provides the capability to augment base models by integrating the proprietary data from your enterprise and fine-tuning them directly within the product. This streamlined process enables the base model's fine-tuning to easily achieve task-specific performance. You can seamlessly bring in models from external commercial sources and open-source models, expanding the range of available models to address your needs.</p> <p>GALE provides you with 3 different options as part of the model studio:</p> <ol> <li>Fine-tuning model: This section, enables you to perform fine-tuning on an existing model, allowing you to create customized models tailored to your specific needs.</li> <li>Open-source model: In this section, you can view all the models deployed by Kore.ai that are available for your usage.</li> <li>External models: In this section, you can add external commercial models. Moreover, if you've hosted any models and wish to integrate them into GALE using API connections, this can be seamlessly achieved in this section.</li> </ol>"},{"location":"models/custom-models/configure-your-custom-model/","title":"Configure your Custom Model","text":""},{"location":"models/custom-models/configure-your-custom-model/#configure-your-custom-model","title":"Configure your custom model","text":"<p>You can modify the general details of your custom model if required.</p> <p>To modify the settings of your custom model, follow the steps:</p> <ol> <li> <p>Click the Configurations tab from the left panel on the Models page. The Configurations page is displayed.</p> <p></p> </li> <li> <p>Make any required changes and the changes are auto-saved.</p> </li> <li>You also have the option to suspend your deployed custom model using the Proceed to undeploy button. It immediately un-deploys the model and is not available for any inferencing requests.</li> </ol> <p>Note</p> <p>To delete a custom model, you must first un-deploy it and then delete it. </p>"},{"location":"models/custom-models/create-a-custom-model/","title":"Create a custom model","text":""},{"location":"models/custom-models/create-a-custom-model/#create-a-custom-model","title":"Create a custom model","text":"<p>You can create a custom model in the Create a custom model wizard, which involves the following 5 steps:</p> <ul> <li>General details</li> <li>Fine-tuning configuration</li> <li>Adding the training and evaluation datasets</li> <li>Adding the test datasets (optional)</li> <li>Integrating with Weights &amp; Biases (optional)</li> </ul> <p>Let us now look into each step in detail.</p> <p>To create a custom model, follow these steps:</p> <ol> <li> <p>Click Models on the top navigation bar of the application. The Models page is displayed.</p> <p>&lt;</p> </li> <li> <p>Click the Create a custom model button on the Models page. The Create a custom model dialog is displayed.</p> </li> <li> <p>In the General details section:</p> <ul> <li> <p>Enter a Name and Description for your custom model.</p> <p></p> </li> <li> <p>Provide tags to ease the search for the model and click Next.</p> </li> </ul> </li> <li> <p>In the Fine-Tuning configuration section:</p> <ul> <li> <p>Select a Base model that you want to fine-tune for your requirements.</p> </li> <li> <p>Enter the Number of Epochs which implies the number of times the entire data set is passed through the model during the training process.</p> <p></p> </li> <li> <p>Enter a number for Batch size which implies the number of training examples used in one iteration of training.</p> </li> <li> <p>Enter a value for the Learning rate which implies the size of the steps taken during the optimization of a model.</p> </li> <li> <p>Click Next to proceed to the next step.</p> </li> </ul> </li> <li> <p>In the Dataset section:</p> <ul> <li>Select the Training dataset which is the data that will be used in training the base model and acts as the foundation for the model's learning.</li> </ul> <p>Note</p> <p>The format accepted is JSONL, CSV, or JSON.</p> <ul> <li> <p>Define the data for evaluation. You can use the same training data set in the evaluation process as well. In this step, you can either allocate a percentage of the training dataset to use for evaluation, or upload a new evaluation dataset, or skip the evaluation.</p> <p></p> </li> <li> <p>Click Next to proceed to the next step.</p> </li> </ul> </li> <li> <p>In the Test data section:</p> <ul> <li>Upload a dataset with which you want to test your fine-tuned model.</li> </ul> <p>Note</p> <p>The format accepted is JSONL, CSV, or JSON.</p> <p></p> <ul> <li>Click Next to proceed to the next step.</li> </ul> <p>Note</p> <p>If you change the training file or the evaluation file in the Dataset section of the wizard, then a warning is displayed in the Test data section. You can verify and proceed.</p> </li> <li> <p>In the Weights &amp; Biases section:</p> <ul> <li>Move the Enable toggle button to integrate with the Weights &amp; Biases platform.  If the toggle button is disabled by default then the data is not shared with the W&amp;B platform even if the integration is set up. You need to enable the toggle button in that case manually.</li> </ul> <p>Note</p> <p>You need an account with Weights and Biases. Enabling the integration with the help of an API token will share your real-time fine-tuning status with the platform and you can comprehensively monitor the fine-tuning metrics of your model. You can use the API token they provided to create an integration and then all the data related to the fine-tuning process will be sent to the account related to that API token.</p> <p></p> <ul> <li>Click Next to proceed to the next step.</li> </ul> </li> <li> <p>In the Review step, verify all the details that you provided earlier.</p> <p>Note</p> <p>If you want to make any modifications, you can go to the previous step by clicking the Back button or a particular step indicator on the left panel.</p> <p></p> </li> <li> <p>Click Start fine-tuning. The Overview page of your custom model is displayed with the status \u201cInitializing\u201d.</p> <p></p> <p>The different statuses that are involved in the process include:</p> <ul> <li> <p>Initializing</p> </li> <li> <p>Training in progress</p> </li> <li> <p>Testing in progress</p> </li> <li> <p>Fine-tuning completed</p> </li> </ul> <p>Note</p> <p>You can stop the process if required in the middle and resume it later, then the status is changed to \u201cStopped\u201d. Then a Re-trigger button is displayed to initiate the fine-tuning process again from the beginning. If the fine-tuning process fails, then the status is changed to \u201cFailed\u201d and you can view the reason for failure. You can make the required changes, and click the Re-trigger button to start the fine-tuning again.</p> <p>Once the Fine-tuning is triggered, you can check the progress in real-time on the **Overview **page. You can click any model name from the list of Fine-tuned models and the Overview page of the model is displayed.</p> <p>This page displays the following sections:</p> <ul> <li> <p>Training information: This section captures crucial information such as the type of training being performed, the step at which the training is currently, training duration, training loss, and validation loss, graphically displays them with a click of arrows, and plots the overall loss trend. You can click the arrow keys to access the graph.</p> </li> <li> <p>Test data information: This section indicates how well your fine-tuned model performed on the test dataset (if you provided any) with the help of a metric called the BLEU score.</p> </li> <li> <p>Training parameters: This section displays the summary of the parameters you have provided in the wizard and it is displayed for your convenience.</p> </li> <li> <p>System information: This section captures the information about the CPUs and GPUs / IT infrastructure at the back-end, that are required in the Fine-tuning process.</p> </li> </ul> </li> </ol> <p>You can also download the training file for your reference from this page. You also have the option to download the test result and the test data from the test info section once the testing is completed.</p> <p>Note</p> <pre><code>If you want to perform the testing again, you can click the Retry option corresponding to the Test info section on the Overview page and select a new test data set file or use the existing file and confirm to start the testing again. The status then is displayed as \u201cTesting in Progress\u201d.\n</code></pre> <p></p> <p>After the model is fine-tuned, you can deploy the custom model and use it across GALE and also for external use.</p> <p>Note</p> <pre><code>When fine-tuning is completed, you can use it to create another custom or fine-tuned model on top of this model.\n</code></pre>"},{"location":"models/custom-models/custom-model-overview/","title":"Custom Models","text":"<p>Fine-tuning a pre-existing model is a common approach in creating custom models. By starting with a pre-trained model, which has its functions and knowledge, and building on top of it, you can save time and resources. Fine-tuning involves training the model on your specific dataset or domain to adapt it to your needs. This way, you can leverage the knowledge and features learned by the pre-trained model while tailoring it to your data and requirements.</p>"},{"location":"models/custom-models/deploy-a-custom-model/","title":"Deploy a Custom Model","text":""},{"location":"models/custom-models/deploy-a-custom-model/#deploy-a-custom-model","title":"Deploy a custom model","text":"<p>Once the fine-tuning process is completed, you can deploy your custom model.</p> <p>To deploy your custom model, follow these steps:</p> <ol> <li> <p>Do one of the following:</p> <ul> <li> <p>Click the Deploy model button on the Overview page of your custom model.</p> <p></p> </li> </ul> <p>Or</p> <ul> <li> <p>Click the Deploy tab in the left panel on the Overview page of your custom model.</p> <p></p> </li> </ul> </li> <li> <p>Click Deploy model. The deployment of your model will start and after the deployment process is complete, you can find the API endpoint created for your custom model. The API endpoint is available in 3 formats as shown in the following image. You can copy and use the same as required.</p> <p></p> <p>After the model is deployed, the API endpoint is generated which implies that your custom model is ready for inferencing externally and across the other sections in GALE.</p> </li> </ol> <p>Note</p> <p>You will receive an email notification after your model deployment is completed and an API is generated and ready to use.</p> <p>You can use the deployed custom model in GALE for the following use cases:</p> <ul> <li> <p>In the Prompt Playground to compare prompts against commercial, open-source, or any other custom model.</p> </li> <li> <p>In an application in the app flow builder via the LLM Node.</p> </li> </ul>"},{"location":"models/custom-models/export-your-custom-model/","title":"Export your Custom Model","text":""},{"location":"models/custom-models/export-your-custom-model/#export-your-custom-model","title":"Export your custom model","text":"<p>You can export your custom model for reference.</p> <p>To export your custom model, follow these steps:</p> <ol> <li> <p>Click the three dots icon corresponding to the Model name on the Models page. A pop-up with a list of options is displayed.</p> <p></p> </li> <li> <p>Click Export model from the list of options. The exported zip file is saved in your downloads folder.</p> </li> </ol>"},{"location":"models/custom-models/generate-an-api-key/","title":"Generate a new API Key","text":""},{"location":"models/custom-models/generate-an-api-key/#generate-a-new-api-key","title":"Generate a new API key","text":"<p>You can generate an API for your custom model and share it with other trusted users. It is essential to have a secure API key when trying to connect to this custom model in an external environment.  </p> <p>To generate a new API key for your custom model, follow these steps:</p> <ol> <li> <p>Click the API keys tab in the left panel on the Overview page of your custom model.</p> <p></p> </li> <li> <p>Click the Create a new API key button. The Create new API key dialog is displayed.</p> <p></p> </li> <li> <p>Enter a Name for the key and click the Generate key button. Your API key is now generated. You can copy the key and share it with others if required. </p> <p>All the generated API keys are listed in the API keys section. You can hover over any key and find the copy icon corresponding to the name of the key. Click the Copy icon and copy the key to share it with others if required to use your custom model.</p> <p></p> </li> </ol>"},{"location":"models/external-models/add-an-external-model/","title":"Add an External Model","text":""},{"location":"models/external-models/add-an-external-model/#add-an-external-model","title":"Add an external model","text":"<p>You can select one option from the 2 options provided to add an external model.</p> <p>For example, here in this topic, you can see the process of adding the Claude-V1 LLM model from the provider Anthropic.</p> <p>To add an external model, follow these steps:</p> <ol> <li>Click Models on the top navigation bar of the application. The Models page is displayed.</li> <li> <p>Click the External models tab on the Models page.</p> <p></p> </li> <li> <p>Click Add a model under the external models tab. The Add an external model dialog is displayed.</p> <p></p> </li> <li> <p>Select the Easy integration option to integrate models from Open AI, Anthropic, Google, or Cohere and click Next.</p> </li> <li> <p>Select a provider to integrate with and click Next.</p> <p></p> <p>A Pop-up with the list of all the Anthropic models that are supported in GALE is displayed.</p> <p></p> </li> <li> <p>Select the required Model and click Next.</p> </li> <li> <p>Enter the respective API key you have received from the provider in the API key field and click Confirm to start the integration.</p> <p></p> <p>The model is integrated and is listed in the External models list.</p> </li> </ol> <p>Note</p> <ul> <li>You can click the 3 dots icon corresponding to the Model name in the list of external models and edit or delete the model.</li> <li>You can set the Inference option using the toggle button corresponding to the Model name. If the Inferencing toggle is ON, you can use this model across GALE. If the toggle button is OFF, it means you cannot infer it anywhere in GALE. For example, in the playground, an error message is displayed that the model is not active even though you have added it in the external models tab.</li> </ul>"},{"location":"models/external-models/external-models-overview/","title":"External Models","text":"<p>GALE offers flexible options for integrating commercial models.</p> <p>The integration is done in 2 ways:</p> <ul> <li>Through the Easy integration method, you can choose a model from a commercial provider like Open AI, Anthropic, or Google and use the respective API key provided by the provider.</li> <li>Alternatively, if you hosted your model externally, GALE provides a custom API integration method to seamlessly incorporate it into the platform.</li> </ul> <p>With these options, GALE empowers you to leverage the full potential of your preferred models while enjoying the benefits of our platform's advanced capabilities.</p>"},{"location":"models/open-source-models/configure-your-open-source-model/","title":"Configure your Open-Source Model","text":""},{"location":"models/open-source-models/configure-your-open-source-model/#configure-your-open-source-model","title":"Configure your open-source model","text":"<p>You can modify the general details of the open-source model if required.</p> <p>To modify the settings of the open-source model, follow the steps:</p> <ol> <li> <p>Click the Configurations tab from the left panel on the Models page. The Configurations page is displayed.</p> <p></p> </li> <li> <p>Make any required changes in the Description, or Tags fields, and the changes are auto-saved.</p> </li> <li>You also have the option to suspend your deployed custom model using the Proceed to undeploy button. It immediately un-deploys the model and is not available for any inferencing requests. Then the status of the model changes to \u201cReady to deploy\u201d. You can deploy the model again from the Deploy section to use it.</li> </ol> <p>Note</p> <p>To delete an open-source model, you must undeploy it and then only you can delete it.</p>"},{"location":"models/open-source-models/generate-an-api-key-open-source/","title":"Generate a new API key","text":""},{"location":"models/open-source-models/generate-an-api-key-open-source/#generate-a-new-api-key","title":"Generate a new API key","text":"<p>You can generate an API key for your open-source model and share it with other trusted users. It is essential to have a secure API key when trying to connect to this open-source model in an external ecosystem.  </p> <p>To generate a new API key for your open-source model, follow these steps:</p> <ol> <li> <p>Click the API keys tab from the left panel on the Models page.</p> <p></p> </li> <li> <p>Click the Create a new API key button. The Create new API key dialog is displayed.</p> <p></p> </li> <li> <p>Enter a Name for the key and click the Generate key button. Your API key is now generated. You can copy the key and share it with others if required.</p> <p>All the generated API keys are listed in the API key section. You can hover over any key and find the copy icon corresponding to the name of the key. Click the icon and copy the key to share it with others if required to use your open-source model.</p> <p></p> </li> </ol>"},{"location":"models/open-source-models/open-source-models-overview/","title":"Open-Source Models","text":"<p>Open-source models refer to the models for which the source code is openly available. You can deploy them in GALE and use them either, throughout GALE, or externally via a generated API endpoint.</p>"},{"location":"models/open-source-models/select-and-deploy-an-open-source-model/","title":"Select and Deploy an Open-Source model","text":""},{"location":"models/open-source-models/select-and-deploy-an-open-source-model/#select-and-deploy-an-open-source-model","title":"Select and deploy an open-source model","text":"<p>Currently, we support thirty-plus open-source models and provide them as a service to our users.</p> <p>To select and deploy a model, follow these steps:</p> <ol> <li>Click Models on the top navigation bar of the application. The Models page is displayed.</li> <li> <p>Click the Open-source models tab on the Models page.</p> <p></p> </li> <li> <p>Click the Deploy a model button. A pop-up with a list of available models is displayed.</p> <p></p> </li> <li> <p>Select a model from the list. For example, you can select the \u201cgoogle/flan-t5-small\u201d model and click the model name. A deployment status alert is displayed stating that there will be inference charges once your deployment starts. Click OK to confirm with deployment.  </p> </li> <li> <p>As you click the model name the deployment process of the model begins automatically, and the status is displayed as \u201cDeploying\u201d in the list of open-source models.</p> <p></p> </li> </ol> <p>Note</p> <p>You will be charged for deployment and inferencing-related costs for each open-source model.</p> <p>After the deployment process is complete the status is changed to \u201cDeployed\u201d. You can now infer this model across GALE and externally.</p> <p>Note</p> <p>You will receive an email notification after your model deployment is completed and an API is generated, and it is ready to use.</p> <p>You can click the required model from the model listing as shown in the preceding image and go to the Manage page. You can find the API endpoint created for this open-source model. To use this model as a service, the generated code is helpful. You can copy it easily as shown in the following image.</p> <p></p> <p>Note</p> <p>Click the three dots icon corresponding to the Model name under the Open-source models tab on the Models page. A pop-up with a list of options is displayed as shown in the following image. If you choose the API endpoint option, it will navigate you to the API keys section of the Model and if you select the Configurations option, it will take you to the Configurations section of the Model.</p> <p></p>"},{"location":"playground/add-a-model/","title":"Add Model","text":"<p>Once you have given your prompts in the Prompt box, you can test your prompts across different AI model(s) and decide on the required model.</p> <p>To add a model, follow these steps:</p> <ol> <li>Click the required Experiment from the Experiments page. The Prompt experiments page is displayed.</li> <li>Click Add Model on the Prompts page under the Data Model section. A pop-up with a list of options is displayed.</li> <li> <p>Click Manage models from the list of options.</p> <p></p> <p>You will be redirected to the External models page to add a model. For more information about how to add an external model, see Add an external model.</p> <p>Note</p> <p>Follow the preceding process of adding an external model only if you are adding any model for the first time. If you have already added a model, it is automatically displayed in the pop-up list when you click the Add model button. For example, if you have added the Claude-v1 model from Anthropic then it is displayed in the pop-up and you click to select the model as shown in the following image.</p> <p></p> <p>You can add other models also to compare and decide which model provides the suitable output for your prompt. </p> </li> <li> <p>In the added model section click the + icon corresponding to the model\u2019s name.\u00a0 A pop-up with a list of options is displayed.</p> <p></p> </li> <li> <p>Repeat step 3 to add another model to compare the outputs. </p> </li> <li>Click the Model setting icon corresponding to the model\u2019s name to make any changes in the settings of the base model you have selected. </li> <li> <p>Click Generate Output after you have added the required number of models to compare. The Generated output for comparison is displayed in the Model section of the Prompt playground space for your reference as shown in the following image.</p> <p></p> <p>Note</p> <p>You can click the response to view the expanded view of the response, click the copy icon corresponding to the response to copy and use the response, make any settings changes in the model to see a difference in the model response, and remove the model if required.</p> </li> </ol>"},{"location":"playground/create-a-new-prompt-experiment/","title":"Create a new prompt experiment","text":"<p>A prompt experiment is a process of testing and comparing the performance of different AI models using a specific input called a prompt. The prompt can be a phrase, a question, or a paragraph of data, and the AI model generates an output based on the input.</p> <p>In a prompt experiment, you can create experiments in either a simple or an advanced mode. In the simple mode, you can easily input prompts and select pre-trained models to automatically generate outputs. In an advanced mode, you have more control over the experiment, allowing you to define your own data sets, add example prompts, and result prompts, and experiment to get the optimized results. </p> <p>Prompt experiments are useful for testing and comparing AI models for specific use cases and applications. </p> <p>To create a prompt experiment, follow these steps:</p> <ol> <li> <p>Click Playground on the top navigation bar of the application. The Experiments page is displayed.</p> <p></p> </li> <li> <p>Click the Create experiment button. The New experiment dialog is displayed.</p> <p></p> </li> <li> <p>Enter a name for your experiment in the Experiment name field.</p> </li> <li>Select the Type of experiment if it is Text-based, then click the Text generation option.</li> <li> <p>Click Create to start with your prompt experiment process. A blank Prompt experiment page is displayed where you can enter your prompt and add models to compare the generated outputs.</p> <p></p> </li> </ol>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/","title":"Explore other actions to perform in the prompt playground","text":"<p>You can also perform other actions on the prompt page and use it for your reference. You can view the history of your experiment and restore the version of the prompt you like, export prompt data, and copy the experiment as a prompt or as an LLM node.</p> <p></p>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#copy-a-prompt","title":"Copy a prompt","text":"<p>You can copy a prompt experiment and use it for your reference for easy access to the other sections in the product.</p> <p>You have 2 options to copy your prompt experiment:</p> <ul> <li>Copy as a prompt: If you select the copy as a prompt option, then the user prompt, a few short examples, and the system prompt are all copied. </li> <li>Copy as LLM node: If you select the copy as LLM node option, then along with the user prompt, a few short examples, and the system prompt, the bookmarked model and the settings of the model section are also copied. Once it is copied you can paste it as an LLM node in the Flow builder canvas as an auto-populated LLM node. </li> </ul> <p>To copy a prompt, follow these steps:</p> <ul> <li> <p>On the Prompts experiment page, click Copy on the header of the page. A pop-up with options is displayed.</p> <p></p> </li> <li> <p>Click as prompt or as LLM node as per your requirement and the experiment is copied.</p> </li> </ul>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#restore-a-prompt-version-using-the-history","title":"Restore a prompt version using the history","text":"<p>You can review the timeline of prompts and their corresponding generated outputs, complete with date and time stamps. Additionally, you can restore a prompt from this section, enabling you to utilize it and implement any necessary modifications, thereby saving considerable time compared to creating a new prompt from scratch.</p> <p>To restore a prompt, follow these steps:</p> <ul> <li> <p>On the Prompts experiment page, click the History icon on the header of the page.</p> <p></p> <p>The history section is displayed with the list of all the old prompt experiments you created.</p> </li> <li> <p>Click the required old version of the experiment. The Restore option is displayed on the footer of the prompt experiment page.</p> <p>Note</p> <p>You can import and copy the older version of the prompt experiment if required.</p> <p></p> </li> <li> <p>Click Restore to view the selected version of the prompt experiment.</p> </li> </ul>"},{"location":"playground/explore-other-actions-to-perform-in-the-prompt-playground/#export-prompt-data","title":"Export prompt data","text":"<p>You can download the prompt and the related experiment data and export it as a CSV file which can be used for later reference.\u00a0</p> <p>To export a CSV file, follow these steps:</p> <ul> <li>On the Prompts experiment page, click the download icon on the header of the page.</li> </ul> <p>The prompt experiment is downloaded as a CSV file successfully and is available in the downloads section of your computer.</p>"},{"location":"playground/generate-output-using-advanced-mode/","title":"Generate output using advanced mode","text":"<p>While using the prompt playground, you can upload a dataset in advanced mode as input. You can add and pre-view the existing or newly uploaded datasets. You can also replace one dataset with another based on the requirement for a particular prompt and generate multiple outputs simultaneously. </p> <p>The advanced mode option helps you improve the prompt creation and testing process more easily and efficiently. You can define a prompt to send to the models, add a few examples for the model to understand the output you are expecting, and upload a data set with inputs. In this mode, you can tweak and refine your prompts, generate the outputs for the selected models, and review the generated responses.\u00a0\u00a0\u00a0</p> <p>On the Prompt page, you can find a Prompt sample auto-populated based on the template you have selected. This is additional information/instruction given to the LLM while it processes the prompt. You can modify this prompt if required.</p> <p>To generate an output using advanced mode, follow these steps:</p> <ol> <li> <p>On the Prompt page, click the Advance Mode toggle button to shift to the advanced mode from the simple mode of output generation.</p> <p></p> <p>The Prompt playground view is enhanced for you with other options to explore.</p> </li> </ol> <p></p> <ol> <li> <p>Click the +Add Dataset link under the Dataset section on the Prompt page to upload your CSV file with data.</p> <p></p> <p>The Dataset dialog is displayed.</p> </li> <li> <p>Click Upload file to upload a CSV file from your local computer and click Upload. You can see a preview of all the data in your CSV file. Click Proceed to accept the file.</p> </li> <li> <p>Click the arrow under the Examples section on the Prompt section to add a sample user input and sample AI response output.</p> <p></p> </li> <li> <p>Click Save to save the sample examples.</p> </li> <li> <p>Click the down arrow corresponding to the Input field. A pop-up with a list of numbers such as 5, 10, 15, and 20 are visible. This displays the number of rows you want to see in your generated output from the CSV file that you have selected. Select the Randomize check box if you want to select randomly otherwise the list is sequential.</p> </li> <li> <p>The value for the variables in the {{}} braces is populated from the uploaded CSV file. You can add variables to the prompt by opening the brackets. For example, {{ xyz }} is a variable in the prompt box.\u00a0</p> <p>The value of the variable is populated from the dataset or file you have uploaded. To make sure the value is mapped correctly between the variable and the file, for the data to flow, you need to make the column name a variable within those curly braces. </p> <p>For example,\u00a0{{Name}} is a variable, from the file you uploaded where Name is a column in that CSV file. A maximum of 20 names or rows that you select can flow from the file to the prompt input column of the advanced mode.</p> </li> <li> <p>Click the + icon corresponding to the Add Model section to add the models you want to select and compare the generated outputs.</p> <p>Note</p> <p>For more information about how to add a model to the prompt, see Add an external model.</p> </li> <li> <p>Click Generate Output after you have added the required number of models to compare. The Generated output for comparison is displayed in the Model section of the Prompt playground space for your reference.</p> <p>Note</p> <p>You can view the matrix in the generated output section which helps in deciding which prompt and model are better for your requirement.</p> <p></p> </li> </ol> <p>It displays the total tokens matrix which implies how many tokens are sent to the model and how many are sent back in the response, the time taken by the model to respond with the answer, and the response and request in the JSON format. You can click the View in JSON format icon at the bottom of the generated output section to view the request and response generated in JSON format in a separate dialog box as shown in the following image.</p> <p></p> <p>If you want to view all the details of the outputs across all the models you can turn ON the toggle button by clicking the info icon on the Models section of the Playground space as shown in the following image. If you turn OFF the toggle, then the total token matrix is not displayed.</p> <p></p>"},{"location":"playground/overview/","title":"Overview","text":"<p>Prompts are the basic inputs you provide to an LLM model to generate a required response. Prompts can also be augmented with additional information, such as examples, to help the model understand the context of the input. This can improve the accuracy and relevance of the output, making it more valuable to the user.</p> <p>Designing effective prompts is critical to training and using LLM. By providing clear and relevant inputs, you can ensure that the models can provide accurate and useful responses to your queries and tasks. The prompt module allows you to experiment with different prompts and models to achieve the desired results.</p>"},{"location":"playground/select-a-template-for-a-prompt/","title":"Select a template for a prompt","text":"<p>You can use the pre-defined templates available in GALE while providing prompts to make your experience easy and save time. There are different categories of templates that you can use to get the desired output. The different categories include analyzing documents, writing and conversation, learning, and coding. In each of the categories, there are a few templates that can be used for a specific task.\u00a0</p> <p>For example, if you select the \u201cWriting a blog post\u201d template from the Writing and Conversations category then the sample prompt is ready in the Prompt box which looks like this: \u201cGenerate a blog post on the topic {{topic}} in a {{ tone }} tone\u201d. Similarly, you can use the \u201cTeach me about GANs\u201d template from the Learning category, and the prompt will look like this: \u201cTeach me about the topic {{ topic name }} like I am a 5-year-old\u201d. You can provide the variables and generate the output for your reference. Variables are the values you provide in the curly braces when selecting a prompt template.</p> <p>Note</p> <p>In the simple mode, you can remove the curly braces and add the required variable, but in the advanced mode, the data comes from the dataset file that you have selected.\u00a0For more information about uploading a dataset, see Upload a dataset file.</p> <p>To select a template, follow these steps:</p> <ol> <li> <p>On the Prompts experiment page, click Templates corresponding to the Prompt box.</p> <p></p> <p>The list of templates is displayed.</p> </li> <li> <p>Click the required template from the category you require. For example, you can take a template \u201cText to bullet points\u201d and then the prompt is displayed in the Prompt box.</p> <p></p> </li> <li> <p>Enter a value for the variables by removing the {{ }} to get the desired output as shown in the following image.</p> <p>Note</p> <p>In the simple mode you must remove the {{}} and add the value in the prompt.</p> <p></p> <p>Once you generate an output, you will be able to view the output as shown in the following image.</p> <p></p> </li> </ol>"},{"location":"temp/test/","title":"Test","text":"Q&amp;A_1 <p>Q&amp;A_1_content</p> Q&amp;A_1 <p>Q&amp;A_1_content</p> <ul> <li> <p> HTML for content and structure</p> </li> <li> <p> JavaScript for interactivity</p> </li> <li> <p> CSS for text running out of boxes</p> </li> <li> <p> Internet Explorer ... huh?</p> </li> </ul> Tab 1Tab 2 <p> HTML for content and structure</p> <p> JavaScript for interactivity</p> <p> CSS for text running out of boxes</p> <p> Internet Explorer ... huh?</p> <p>Test  <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> <p>End of tab</p> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes  Getting started</p> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site  Reference</p> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines  Customization</p> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]  License</p> <p> HTML for content and structure</p> <p> JavaScript for interactivity</p> <p> CSS for text running out of boxes</p> <p> Internet Explorer New ... huh?</p> <p> Virtual Assistant Integration The platform offers robust integration capabilities, enabling seamless connection with various systems and services. This ensures that your virtual assistant can work in harmony with your existing tools and infrastructure. Learn More Omnichannel Capabilities The platform provides omnichannel support, allowing your virtual assistant to engage with users across multiple channels such as web, mobile, and messaging apps. This ensures consistent and efficient customer interactions. Learn More Multilingual Support Kore.ai\u2019s virtual assistant is equipped with multilingual capabilities, enabling it to understand and communicate in different languages. This feature helps you cater to a diverse user base and expand your global reach. Learn More </p> Tab 1Tab 2 <p> Virtual Assistant Integration The platform offers robust integration capabilities, enabling seamless connection with various systems and services. This ensures that your virtual assistant can work in harmony with your existing tools and infrastructure. Learn More Omnichannel Capabilities The platform provides omnichannel support, allowing your virtual assistant to engage with users across multiple channels such as web, mobile, and messaging apps. This ensures consistent and efficient customer interactions. Learn More Multilingual Support Kore.ai\u2019s virtual assistant is equipped with multilingual capabilities, enabling it to understand and communicate in different languages. This feature helps you cater to a diverse user base and expand your global reach. Learn More </p> <p> Introduction to Dialog Tasks A core component of the XO Platform, an essential tool for building conversations that are connected to your business logic. Reference Introduction to Dialog Tasks A core component of the XO Platform,an essential tool for building conversations that are connected to your business logic. Reference Introduction to Dialog Tasks A core component of the XO Platform,an essential tool for building conversations that are connected to your business logic. Reference </p> <p> Define Use Cases: Use Cases are fundamental building blocks for automation. Build use cases to help specific audiences navigate well-defined steps that solve a clear intent. Learn More </p> <p> Leverage Natural Language: Leverage the Platform\u2019s NLP capabilities to train and optimize your automation workflows to handle complex use cases. Learn More </p> <p> Intelligence Management: Empower your automation workflows to handle nuances of human conversations, including interruptions, clarifications, and more. Learn More </p> <p> Test Workflows: Use an extensive suite of features to conduct rigorous testing of your automation workflows to ensure everything works as expected. Find and fix problems before they reach your users. Learn More </p>"},{"location":"temp/whats-new-in-this-release/","title":"What\u2019s New","text":"<p>Learn about the new features and enhancements included in v10.1 of Kore.ai Experience Optimization Platform which was released on April 16, 2023.</p> <p>The v10.1 of the Kore.ai XO Platform focuses on leveraging the power of Large Language Models and Generative AI to enable enterprises to create intelligent and context-specific conversational experiences. The release offers a copilot for smart assistance, better conversational capabilities, and delivers personalized responses.</p> <p>The key features and enhancements included in this release are listed below for your reference:</p>"},{"location":"temp/whats-new-in-this-release/#smart-copilot-for-iva-development","title":"Smart Copilot for IVA Development","text":""},{"location":"temp/whats-new-in-this-release/#enhanced-bot-creation-journey-with-use-case-suggestions","title":"Enhanced Bot Creation Journey with Use Case Suggestions","text":"<p>Create Virtual Assistants faster with the new bot creation process that lets you generate use cases automatically. Dialog Tasks are auto-created along with the bot, providing you with the base framework to fasttrack your VA creation journey. [Learn more].</p>"},{"location":"temp/whats-new-in-this-release/#automatic-dialog-generation","title":"Automatic Dialog Generation","text":"<p>This feature auto-generates conversations and dialog flows using the VA\u2019s purpose and intent description provided during the creation process. The Platform uses LLM and generative AI to create suitable Dialog Tasks for Conversation Design, Logic Building &amp; Training by including the required nodes in the flow.</p> <p>You can provide an intent description, and the Platform handles the Conversation Generation for the Dialog Flow. You can preview the conversation flow, view the Bot Action taken, improvise the intent description, and regenerate the conversation to make it more human-like. The nodes and the flow for the Business Logic are automatically built for your conversation, and you only need to configure the flow transition. Learn more .</p> <p></p>"},{"location":"temp/whats-new-in-this-release/#training-data-suggestions","title":"Training Data Suggestions","text":"<p>Quickly generate high-quality training data using suggested utterances for each intent. Review and add the utterances to create a robust training set for your bot.  Learn more .</p> <p></p>"},{"location":"temp/whats-new-in-this-release/#nlp-batch-test-case-suggestions","title":"NLP Batch Test Case Suggestions","text":"<p>Automatically generate NLP test cases for every intent, including the entity checks. Use the generated utterances to quickly create test suites in the builder.  Learn more .</p> <p></p>"},{"location":"temp/whats-new-in-this-release/#conversation-test-cases-suggestions","title":"Conversation Test Cases Suggestions","text":"<p>Get simulated user inputs covering end-user scenarios at every test step. Use the suggestions to create test suites instantly. You can view input/utterance suggestions at every conversation step simulating the various input types and scenarios. This feature helps check if the task/intent is robust enough to handle random user utterances. It helps you predict and simulate the end user\u2019s behavior and check if the VA can execute all the defined flows by generating user responses and presenting any digressions from the specified intent.  Learn more .</p> <p></p>"},{"location":"temp/whats-new-in-this-release/#dynamic-conversations","title":"Dynamic Conversations","text":""},{"location":"temp/whats-new-in-this-release/#dynamic-paraphrasing","title":"Dynamic Paraphrasing","text":"<p>You can now leverage Generative AI to rephrase bot responses based on conversation context and users\u2019 emotions, resulting in more empathetic and natural responses that enhance user experience and engagement. When the OpenAI or Azure-OpenAI integration is enabled, you can see a new setting to rephrase responses at the node level for Message, Entity, and Confirmation Nodes. The messages added as User Prompts, Error Prompts, and Bot Responses are rephrased during runtime using the integration.  Learn more .</p> <p></p>"},{"location":"temp/whats-new-in-this-release/#ai-assisted-adaptive-dialog","title":"AI-Assisted Adaptive Dialog","text":"<p>The AI-Assisted Dialog Node lets you leverage the full potential of LLMs and Generative AI models to quickly build conversations that involve complex flows and also provide human-like experiences. You can define the entities you would like to collect and also the business rules that govern the collection of these entities. The XO Platform orchestrates the conversation using contextual intelligence, ensuring that the conversation is always grounded to your enterprise business rules. You can also provide exit rules for handing off the conversation to the virtual assistant or the human agents.  Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#knowledge-graph-powered-by-llm","title":"Knowledge Graph Powered by LLM","text":"<p>The new Few-Shot Knowledge Graph leverages LLMs to understand the FAQs without the need for exhaustive ontology. All you need to do is add all FAQs to the root node/term. This significantly reduces the complexity of building and maintaining an ontology structure. Learn more.</p> <p>With the introduction of Few-shot models in ML and KG engines, rescoring by Ranking &amp; Resolver is no longer required for intent identification. Therefore, we have introduced a new version of Ranking &amp; Resolver (Version 2) for Few-shot models that only ranks intents based on scores from ML and KG engines. The version significantly improves the accuracy of intent identification.</p> <p>Multi-language Support for Zero-shot and Few-shot Models</p> <p>The Zero-shot and the Few-shot ML Models are now supported in all non-English languages.</p>"},{"location":"temp/whats-new-in-this-release/#pre-built-integrations","title":"Pre-built Integrations","text":""},{"location":"temp/whats-new-in-this-release/#integration-with-unblu-for-agent-transfer","title":"Integration with Unblu for Agent Transfer","text":"<p>Kore.ai\u2019s pre-built Agent Transfer integrations now allow you to seamlessly hand off the conversations to the Unblu agent system without writing any custom code using the BotKit. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#disable-or-delete-pre-built-action-integrations","title":"Disable or Delete Pre-built Action Integrations","text":"<p>You can now disable or delete a configured external Action integration.</p>"},{"location":"temp/whats-new-in-this-release/#additional-enhancements","title":"Additional Enhancements","text":""},{"location":"temp/whats-new-in-this-release/#external-nlu-for-universal-bots","title":"External NLU for Universal Bots","text":"<p>The Platform provides more flexibility in bot orchestration by allowing you to link some bots using external NLU engines and others using proprietary multi-engine NLP. With external NLU integration, you can continue to have the NLU training on an external system and build the conversation flow on the XO platform. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#analytics-details-in-webhook-v2-response","title":"Analytics Details in Webhook v2 Response","text":"<p>The Webhook v2 response now includes additional conversation flow-related analytics information to help you build custom analytics. You can set the AnalyticsDetails parameter to Include in the request to view meta tags related to all nodes, tasks, sub-task, and session_id details in the response. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#export-conversational-path-analysis-data","title":"Export Conversational Path Analysis Data","text":"<p>You can now export the Conversational Path Analysis data in a .csv file to analyze the flow of drop-off sessions and take the required actions. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#customize-authorization-profiles-for-a-conversion-session","title":"Customize Authorization Profiles for a Conversion Session","text":"<p>The Platform provides a new way to clear authorization profiles collected from the users. The authorization profiles can be auto-cleared as part of the session closure, or you can also use the koreUtil.clearAuthProfiles and koreUtil.clearAllAuthProfiles functions. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#improved-messaging-channel-support","title":"Improved Messaging Channel Support","text":"<p>SmartAssist Gateway is Kore.ai\u2019s native voice interface to deliver seamless and low-latency conversation experiences. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#plan-usage","title":"Plan &amp; Usage","text":"<p>Ecommerce Plans are now supported in the EU region. Also, wire transfer-based payments are now supported for Ecommerce Plans. Learn more.</p>"},{"location":"temp/whats-new-in-this-release/#entity-rule-enhancements","title":"Entity Rule Enhancements","text":"<ul> <li>Letters rule for the String Entity: The new Letters rule <code>(letters=a single number OR a range)</code> for the String entity allows you to extract a word of a specific length or a sequence of individual characters that meet the length criteria. Learn more.</li> <li>New Model Number Rule for Composite Entity: The new Model Number rule <code>(\"modelNumber\":true)</code> for Composite Entity allows you to set a specific number of letters and numbers to extract a unique identifier. For example, a social security number, membership ID, or other structured data on a voice channel where the user\u2019s utterance does not follow a strict regex pattern. Learn more.</li> <li>New Precondition Rule for all entity types allows you to define preconditions for entity extraction if one of the conditions is true. If the precondition is invalid then the entity extraction is skipped entirely. For example, a composite entity matches with a set of identification numbers, such as membership ID, provider ID, and RX number. You can set the precondition rule as <code>\"preConditions\" : [\"checkMemberID\"]</code>. Learn more.</li> </ul>"}]}